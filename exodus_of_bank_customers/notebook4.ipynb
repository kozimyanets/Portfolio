{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отток клиентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из «Бета-Банка» стали уходить клиенты. Каждый месяц. Немного, но заметно. Банковские маркетологи посчитали: сохранять текущих клиентов дешевле, чем привлекать новых.\n",
    "\n",
    "Нужно спрогнозировать, уйдёт клиент из банка в ближайшее время или нет. Вам предоставлены исторические данные о поведении клиентов и расторжении договоров с банком. \n",
    "\n",
    "Постройте модель с предельно большим значением *F1*-меры. Чтобы сдать проект успешно, нужно довести метрику до 0.59. Проверьте *F1*-меру на тестовой выборке самостоятельно.\n",
    "\n",
    "Дополнительно измеряйте *AUC-ROC*, сравнивайте её значение с *F1*-мерой.\n",
    "\n",
    "Источник данных: [https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from category_encoders) (0.13.5)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from category_encoders) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from category_encoders) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from category_encoders) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from category_encoders) (1.1.3)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from category_encoders) (1.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from statsmodels>=0.9.0->category_encoders) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from packaging>=21.3->statsmodels>=0.9.0->category_encoders) (3.0.9)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from imbalanced-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from imbalanced-learn) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\envs\\practicum\\lib\\site-packages (from imbalanced-learn) (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "#Загрузим библиотеку, не входящую во многие распространенные сборки\n",
    "!pip install category_encoders\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим необходимые библиотеки\n",
    "from category_encoders import TargetEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', '.*One or more .*', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>15574012</td>\n",
       "      <td>Chu</td>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8.0</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>15592531</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>15656148</td>\n",
       "      <td>Obinna</td>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>15792365</td>\n",
       "      <td>He</td>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4.0</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>15592389</td>\n",
       "      <td>H?</td>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "5          6    15574012       Chu          645     Spain    Male   44   \n",
       "6          7    15592531  Bartlett          822    France    Male   50   \n",
       "7          8    15656148    Obinna          376   Germany  Female   29   \n",
       "8          9    15792365        He          501    France    Male   44   \n",
       "9         10    15592389        H?          684    France    Male   27   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "5     8.0  113755.78              2          1               0   \n",
       "6     7.0       0.00              2          1               1   \n",
       "7     4.0  115046.74              4          1               0   \n",
       "8     4.0  142051.07              2          0               1   \n",
       "9     2.0  134603.88              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  \n",
       "5        149756.71       1  \n",
       "6         10062.80       0  \n",
       "7        119346.88       1  \n",
       "8         74940.50       0  \n",
       "9         71725.73       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим данные и посмотрим первые строки\n",
    "df = pd.read_csv('/datasets/Churn.csv')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, первый взгляд на данные позволяет сделать несколько выводов.\n",
    "Во-первых, столбцы названы не в змеином стиле. На исследование это, конечно, не влияет, но переименуем их, чтобы было привычней.\n",
    "Во-вторых, есть несколько столбцов, которые едва ли могут повлиять на оценку вероятности ухода клиента из банка. Это «RowNumber», «CustomerId», «Surname». Их удалим.\n",
    "Теоретически по столбцу «Surname» можно определить с определенной долей уверенности национальность клиента, однако это не так просто, а влияние этого возможного признака на точность модели может оказаться минимальным. По этой причине это поле все же удалено.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменим названия столбцов\n",
    "df.columns = ['row_number', 'customer_id', 'surname', 'credit_score', 'geography', 'gender', 'age', 'tenure', 'balance', 'num_of_products', 'has_cr_card', 'is_active_member', 'estimated_salary', 'exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим ненужные столбцы\n",
    "df = df.drop(['row_number', 'customer_id', 'surname'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим типы оставшихся столбцов и полноту данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   credit_score      10000 non-null  int64  \n",
      " 1   geography         10000 non-null  object \n",
      " 2   gender            10000 non-null  object \n",
      " 3   age               10000 non-null  int64  \n",
      " 4   tenure            9091 non-null   float64\n",
      " 5   balance           10000 non-null  float64\n",
      " 6   num_of_products   10000 non-null  int64  \n",
      " 7   has_cr_card       10000 non-null  int64  \n",
      " 8   is_active_member  10000 non-null  int64  \n",
      " 9   estimated_salary  10000 non-null  float64\n",
      " 10  exited            10000 non-null  int64  \n",
      "dtypes: float64(3), int64(6), object(2)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть 10 столбцов с признаками и один – с целевым показателем. Отсутствующие данные есть лишь в столбце «tenure». Одна из следующих задач – попытаться выяснить причины пропусков и выбрать способ работы с ними.\n",
    "Также мы видим две категориальные переменные – «geography» и «gender». Их преобразуем прямым кодированием.\n",
    "Заметим, что тип int64 явно излишен для многих столбцов. В данном случае преобразовывать типы не будем, так как набор данных маленький и в будущем, возможно, потребуется масштабирование признаков.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>591</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140469.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>550</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103391.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90878.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>585</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146050.97</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86424.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>655</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125561.97</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164040.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>742</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136857.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84509.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>543</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26019.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>652</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>114675.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>730</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85982.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>413</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6534.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>538</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108055.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27231.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>684</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126384.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>198129.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>432</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152603.45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110265.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>635</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138296.94</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141075.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>800</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108007.36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47125.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>578</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169462.09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112187.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     credit_score geography  gender  age  tenure    balance  num_of_products  \\\n",
       "30            591     Spain  Female   39     NaN       0.00                3   \n",
       "48            550   Germany    Male   38     NaN  103391.38                1   \n",
       "51            585   Germany    Male   36     NaN  146050.97                2   \n",
       "53            655   Germany    Male   41     NaN  125561.97                1   \n",
       "60            742   Germany    Male   35     NaN  136857.00                1   \n",
       "82            543    France  Female   36     NaN       0.00                2   \n",
       "85            652     Spain  Female   75     NaN       0.00                2   \n",
       "94            730     Spain    Male   42     NaN       0.00                2   \n",
       "99            413    France    Male   34     NaN       0.00                2   \n",
       "111           538   Germany    Male   39     NaN  108055.10                2   \n",
       "123           684   Germany  Female   48     NaN  126384.42                1   \n",
       "125           432    France    Male   42     NaN  152603.45                1   \n",
       "146           635     Spain  Female   29     NaN  138296.94                2   \n",
       "162           800    France  Female   49     NaN  108007.36                1   \n",
       "173           578    France    Male   30     NaN  169462.09                1   \n",
       "\n",
       "     has_cr_card  is_active_member  estimated_salary  exited  \n",
       "30             1                 0         140469.38       1  \n",
       "48             0                 1          90878.13       0  \n",
       "51             0                 0          86424.57       0  \n",
       "53             0                 0         164040.94       1  \n",
       "60             0                 0          84509.57       0  \n",
       "82             0                 0          26019.59       0  \n",
       "85             1                 1         114675.75       0  \n",
       "94             0                 1          85982.47       0  \n",
       "99             0                 0           6534.18       0  \n",
       "111            1                 0          27231.26       0  \n",
       "123            1                 1         198129.36       0  \n",
       "125            1                 0         110265.24       1  \n",
       "146            1                 0         141075.51       0  \n",
       "162            0                 0          47125.11       0  \n",
       "173            1                 0         112187.11       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим, есть ли что-то особенное в наблюдениях, где не заполнен столбец \"tenure\"\n",
    "df[df['tenure'].isna()].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные примеры не позволяют определить закономерности в появлении пропусков. Проверим, возможно интересующий нас столбец имеет сильную связь с каким-то другим признаком в наборе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18100\\3697804709.py:1: FutureWarning: The default value of numeric_only in DataFrame.corrwith is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df.corrwith(df['tenure'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "credit_score       -0.000062\n",
       "age                -0.013134\n",
       "tenure              1.000000\n",
       "balance            -0.007911\n",
       "num_of_products     0.011979\n",
       "has_cr_card         0.027232\n",
       "is_active_member   -0.032178\n",
       "estimated_salary    0.010520\n",
       "exited             -0.016761\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corrwith(df['tenure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весьма красноречиво! Линейных связей нет. Заместим отсутствующие данные медианой. Этот вариант позволяет сохранить данные и минимизировать влияние на работу модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tenure'] = df['tenure'].fillna(df['tenure'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее посмотрим на связь между имеющимися признаками на основе матрицы корреляции. При этом важно помнить, что часть признаков имеет явно бинарный характер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18100\\2953211560.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  sns.heatmap(df.corr().round(2), annot=True);\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.heatmap(df.corr().round(2), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициенты корреляции не демонстрируют значимые линейные связи между показателями. Соответственно, нет никаких оснований удалять те или иные столбцы.\n",
    "Теперь проверим таблицу на наличие явных дубликатов, а затем посмотрим основные характеристики имеющихся признаков.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>4.99790</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.76001</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       credit_score           age       tenure        balance  \\\n",
       "count  10000.000000  10000.000000  10000.00000   10000.000000   \n",
       "mean     650.528800     38.921800      4.99790   76485.889288   \n",
       "std       96.653299     10.487806      2.76001   62397.405202   \n",
       "min      350.000000     18.000000      0.00000       0.000000   \n",
       "25%      584.000000     32.000000      3.00000       0.000000   \n",
       "50%      652.000000     37.000000      5.00000   97198.540000   \n",
       "75%      718.000000     44.000000      7.00000  127644.240000   \n",
       "max      850.000000     92.000000     10.00000  250898.090000   \n",
       "\n",
       "       num_of_products  has_cr_card  is_active_member  estimated_salary  \\\n",
       "count     10000.000000  10000.00000      10000.000000      10000.000000   \n",
       "mean          1.530200      0.70550          0.515100     100090.239881   \n",
       "std           0.581654      0.45584          0.499797      57510.492818   \n",
       "min           1.000000      0.00000          0.000000         11.580000   \n",
       "25%           1.000000      0.00000          0.000000      51002.110000   \n",
       "50%           1.000000      1.00000          1.000000     100193.915000   \n",
       "75%           2.000000      1.00000          1.000000     149388.247500   \n",
       "max           4.000000      1.00000          1.000000     199992.480000   \n",
       "\n",
       "             exited  \n",
       "count  10000.000000  \n",
       "mean       0.203700  \n",
       "std        0.402769  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главная особенность в данных – серьезный дисбаланс целевого показателя. Это важно для формирования тестовой выборки, создания модели.\n",
    "В остальном ничего примечательного в данных не заметно. Большие хвосты есть в возрасте, но ничего удивительного в них нет.\n",
    "Выделим признаки и целевой показатель, разделим данные на обучающую и тестовую выборки. Также необходимо перекодировать категориальные признаки. Это мы сделаем позднее, так как в пайплане можно будет попробовать разные варианты кодировки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отделяем признаки от целевого показателя\n",
    "features = df.drop('exited', axis=1)\n",
    "target = df['exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отделяем 20 % выборки для будущих тестов\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=12345, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 10) (2000, 10) (8000,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Проверим размеры наборов\n",
    "print(features_train.shape, features_test.shape, target_train.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Выводы*\n",
    "Нам удалось загрузить и познакомиться с данными. Были найдены 3 столбца, которые в дальнейшем не понадобятся. Их мы удалили. Также был найден столбец с пропусками. Для пропущенных данных в качестве значения установлена медиана, что позволило не терять данные и минимизировать влияние на результаты моделей. Корреляции между признаками не обнаружено.\n",
    "Переходим к обучению первых моделей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследование задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом этапе мы попробуем обучить разные модели, не учитывая несбалансированность целевого показателя. Тестовую выборку оставим на будущее, при неообходимости используя кросс-валидацию. \n",
    "Сначала посмотрим, каково качество модели, случайно предсказывающей ответы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.28319856244384545\n",
      "roc_auc_score 0.5070470283441362\n"
     ]
    }
   ],
   "source": [
    "model = DummyClassifier(strategy='uniform')\n",
    "model.fit(features_train, target_train)\n",
    "print('f1_score', f1_score(target_train, model.predict(features_train)))\n",
    "print('roc_auc_score', roc_auc_score(target_train, model.predict(features_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое значение получено. Попробуем его улучшить. Теперь посмотрим как проявят себя логистическая регрессия, случайный лес и метод ближайших соседей. Так как это пробная проверка без учета несбалансированности, настраивать гиперпараметры не будем. Но сначала подготовим два преобразователя данных. Более эффективный мы выберем позднее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_target = ColumnTransformer(\n",
    "    [('te', TargetEncoder(), make_column_selector(dtype_include=object)),\n",
    "    ('numbers', StandardScaler(), make_column_selector(dtype_include=np.number))],\n",
    "    remainder='passthrough')\n",
    "ct_ohe = ColumnTransformer(\n",
    "    [('ohe', OneHotEncoder(drop='first', sparse=False), make_column_selector(dtype_include=object)),\n",
    "    ('numbers', StandardScaler(), make_column_selector(dtype_include=np.number))],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат логистической регрессии с target_encoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fit_time        0.149517\n",
       "score_time      0.054687\n",
       "test_f1         0.293749\n",
       "test_roc_auc    0.758170\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат логистической регрессии с OHE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fit_time        0.063834\n",
       "score_time      0.039725\n",
       "test_f1         0.316145\n",
       "test_roc_auc    0.760637\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_target),\n",
    "    ('lr', LogisticRegression(random_state=1235, solver='liblinear'))]\n",
    "    )\n",
    "print('Результат логистической регрессии с target_encoder')\n",
    "display(pd.DataFrame(cross_validate(model, features_train, target_train, scoring=['f1', 'roc_auc'])).mean())\n",
    "\n",
    "model = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('lr', LogisticRegression(random_state=1235, solver='liblinear'))]\n",
    "    )\n",
    "print('Результат логистической регрессии с OHE')\n",
    "display(pd.DataFrame(cross_validate(model, features_train, target_train, scoring=['f1', 'roc_auc'])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат случайного леса с target_encoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fit_time        2.158012\n",
       "score_time      0.242625\n",
       "test_f1         0.563737\n",
       "test_roc_auc    0.847884\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат случайного леса с OHE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fit_time        2.019088\n",
       "score_time      0.220020\n",
       "test_f1         0.567978\n",
       "test_roc_auc    0.847125\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_target),\n",
    "    ('rf', RandomForestClassifier(random_state=1235))]\n",
    "    )\n",
    "print('Результат случайного леса с target_encoder')\n",
    "display(pd.DataFrame(cross_validate(model, features_train, target_train, scoring=['f1', 'roc_auc'])).mean())\n",
    "\n",
    "model = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('rf', RandomForestClassifier(random_state=1235))]\n",
    "    )\n",
    "print('Результат случайного леса с OHE')\n",
    "display(pd.DataFrame(cross_validate(model, features_train, target_train, scoring=['f1', 'roc_auc'])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат метода ближайших соседей с target_encoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fit_time        0.166344\n",
       "score_time      0.553853\n",
       "test_f1         0.497878\n",
       "test_roc_auc    0.769731\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат метода ближайших соседей с OHE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fit_time        0.078228\n",
       "score_time      0.562155\n",
       "test_f1         0.517070\n",
       "test_roc_auc    0.777385\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_target),\n",
    "    ('knn', KNeighborsClassifier())]\n",
    "    )\n",
    "print('Результат метода ближайших соседей с target_encoder')\n",
    "display(pd.DataFrame(cross_validate(model, features_train, target_train, scoring=['f1', 'roc_auc'])).mean())\n",
    "\n",
    "model = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('knn', KNeighborsClassifier())]\n",
    "    )\n",
    "print('Результат метода ближайших соседей с OHE')\n",
    "display(pd.DataFrame(cross_validate(model, features_train, target_train, scoring=['f1', 'roc_auc'])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Выводы*\n",
    "Пока явным фаворитом выглядит случайный лес. Дальше посмотрим, как все эти модели поведут себя после устранения дисбаланса в обучающей выборке.\n",
    "Также обратим внимание на связь f1 и roc_auc. В целом они изменяются однонаправленно – если для модели удается повысить одну характеристику, то и вторая вырастет. Но связь не абсолютная. В каких-то случаях один показатель растет быстрее, а в каких-то – другой.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Борьба с дисбалансом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы видели, что есть заметный дисбаланс целевого показателя. Чтобы уменьшить его влияние на модель расширим выборку, выровняв количество наблюдений разных классов. Подбирать гиперпараметры мы будем с помощью GridSearchCV. Чтобы не допустить утечки информации, расширение будет производится только на фолдах, относящихся к тренировочной части датасета. Как раз с создания генератора фолдов и начнем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = StratifiedKFold(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем логистическую регрессию с двумя вариантами кодирования категориальных переменных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат логистической регрессии с target_encoder\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_lr__C</th>\n",
       "      <th>param_lr__penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.397545</td>\n",
       "      <td>0.055264</td>\n",
       "      <td>0.026166</td>\n",
       "      <td>0.003821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.481879</td>\n",
       "      <td>0.486702</td>\n",
       "      <td>0.489527</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>1</td>\n",
       "      <td>0.483721</td>\n",
       "      <td>0.495873</td>\n",
       "      <td>0.498828</td>\n",
       "      <td>0.492807</td>\n",
       "      <td>0.006537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.463373</td>\n",
       "      <td>0.091259</td>\n",
       "      <td>0.028145</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.506876</td>\n",
       "      <td>0.472462</td>\n",
       "      <td>0.485010</td>\n",
       "      <td>0.488116</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>2</td>\n",
       "      <td>0.493042</td>\n",
       "      <td>0.495743</td>\n",
       "      <td>0.499496</td>\n",
       "      <td>0.496094</td>\n",
       "      <td>0.002647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.194084</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>0.026458</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>2.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 2.0, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.500664</td>\n",
       "      <td>0.478930</td>\n",
       "      <td>0.483079</td>\n",
       "      <td>0.487558</td>\n",
       "      <td>0.009421</td>\n",
       "      <td>3</td>\n",
       "      <td>0.484266</td>\n",
       "      <td>0.493268</td>\n",
       "      <td>0.496978</td>\n",
       "      <td>0.491504</td>\n",
       "      <td>0.005337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.401344</td>\n",
       "      <td>0.082643</td>\n",
       "      <td>0.048897</td>\n",
       "      <td>0.031618</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.498674</td>\n",
       "      <td>0.478780</td>\n",
       "      <td>0.485129</td>\n",
       "      <td>0.487527</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>4</td>\n",
       "      <td>0.488303</td>\n",
       "      <td>0.493268</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.491635</td>\n",
       "      <td>0.002356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.193942</td>\n",
       "      <td>0.010772</td>\n",
       "      <td>0.025443</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.499670</td>\n",
       "      <td>0.478723</td>\n",
       "      <td>0.482895</td>\n",
       "      <td>0.487096</td>\n",
       "      <td>0.009053</td>\n",
       "      <td>5</td>\n",
       "      <td>0.487724</td>\n",
       "      <td>0.491609</td>\n",
       "      <td>0.492482</td>\n",
       "      <td>0.490605</td>\n",
       "      <td>0.002068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.194383</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.024541</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.497026</td>\n",
       "      <td>0.481703</td>\n",
       "      <td>0.480896</td>\n",
       "      <td>0.486542</td>\n",
       "      <td>0.007421</td>\n",
       "      <td>6</td>\n",
       "      <td>0.485367</td>\n",
       "      <td>0.494063</td>\n",
       "      <td>0.495540</td>\n",
       "      <td>0.491657</td>\n",
       "      <td>0.004488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.430220</td>\n",
       "      <td>0.008762</td>\n",
       "      <td>0.026733</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 2.0, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.475366</td>\n",
       "      <td>0.482485</td>\n",
       "      <td>0.485950</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>7</td>\n",
       "      <td>0.487002</td>\n",
       "      <td>0.492489</td>\n",
       "      <td>0.495979</td>\n",
       "      <td>0.491823</td>\n",
       "      <td>0.003695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.192374</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.028144</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.488342</td>\n",
       "      <td>0.476933</td>\n",
       "      <td>0.482213</td>\n",
       "      <td>0.482496</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>8</td>\n",
       "      <td>0.478147</td>\n",
       "      <td>0.486292</td>\n",
       "      <td>0.480782</td>\n",
       "      <td>0.481741</td>\n",
       "      <td>0.003394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.179867</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>0.025695</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.484809</td>\n",
       "      <td>0.472010</td>\n",
       "      <td>0.463430</td>\n",
       "      <td>0.473417</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>9</td>\n",
       "      <td>0.470626</td>\n",
       "      <td>0.473548</td>\n",
       "      <td>0.471534</td>\n",
       "      <td>0.471903</td>\n",
       "      <td>0.001221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.172665</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>0.058561</td>\n",
       "      <td>0.041655</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.479102</td>\n",
       "      <td>0.469151</td>\n",
       "      <td>0.463677</td>\n",
       "      <td>0.470643</td>\n",
       "      <td>0.006385</td>\n",
       "      <td>10</td>\n",
       "      <td>0.470958</td>\n",
       "      <td>0.472421</td>\n",
       "      <td>0.474162</td>\n",
       "      <td>0.472514</td>\n",
       "      <td>0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.170311</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.019402</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.480149</td>\n",
       "      <td>0.460980</td>\n",
       "      <td>0.457711</td>\n",
       "      <td>0.466280</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>11</td>\n",
       "      <td>0.465159</td>\n",
       "      <td>0.472716</td>\n",
       "      <td>0.468991</td>\n",
       "      <td>0.468955</td>\n",
       "      <td>0.003085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.163612</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.032107</td>\n",
       "      <td>0.007528</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.468061</td>\n",
       "      <td>0.458920</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>0.464838</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>12</td>\n",
       "      <td>0.463214</td>\n",
       "      <td>0.467802</td>\n",
       "      <td>0.463576</td>\n",
       "      <td>0.464864</td>\n",
       "      <td>0.002083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_lr__C  \\\n",
       "8        0.397545      0.055264         0.026166        0.003821         1.0   \n",
       "6        0.463373      0.091259         0.028145        0.002805         0.5   \n",
       "11       0.194084      0.010132         0.026458        0.003887         2.0   \n",
       "4        0.401344      0.082643         0.048897        0.031618         0.1   \n",
       "9        0.193942      0.010772         0.025443        0.002658         1.0   \n",
       "7        0.194383      0.004977         0.024541        0.002691         0.5   \n",
       "10       0.430220      0.008762         0.026733        0.004388         2.0   \n",
       "5        0.192374      0.015700         0.028144        0.002860         0.1   \n",
       "3        0.179867      0.016747         0.025695        0.006325        0.01   \n",
       "2        0.172665      0.004289         0.058561        0.041655        0.01   \n",
       "1        0.170311      0.005540         0.019402        0.002666       0.001   \n",
       "0        0.163612      0.006395         0.032107        0.007528       0.001   \n",
       "\n",
       "   param_lr__penalty                                 params  \\\n",
       "8                 l1    {'lr__C': 1.0, 'lr__penalty': 'l1'}   \n",
       "6                 l1    {'lr__C': 0.5, 'lr__penalty': 'l1'}   \n",
       "11                l2    {'lr__C': 2.0, 'lr__penalty': 'l2'}   \n",
       "4                 l1    {'lr__C': 0.1, 'lr__penalty': 'l1'}   \n",
       "9                 l2    {'lr__C': 1.0, 'lr__penalty': 'l2'}   \n",
       "7                 l2    {'lr__C': 0.5, 'lr__penalty': 'l2'}   \n",
       "10                l1    {'lr__C': 2.0, 'lr__penalty': 'l1'}   \n",
       "5                 l2    {'lr__C': 0.1, 'lr__penalty': 'l2'}   \n",
       "3                 l2   {'lr__C': 0.01, 'lr__penalty': 'l2'}   \n",
       "2                 l1   {'lr__C': 0.01, 'lr__penalty': 'l1'}   \n",
       "1                 l2  {'lr__C': 0.001, 'lr__penalty': 'l2'}   \n",
       "0                 l1  {'lr__C': 0.001, 'lr__penalty': 'l1'}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "8            0.500000           0.481879           0.486702         0.489527   \n",
       "6            0.506876           0.472462           0.485010         0.488116   \n",
       "11           0.500664           0.478930           0.483079         0.487558   \n",
       "4            0.498674           0.478780           0.485129         0.487527   \n",
       "9            0.499670           0.478723           0.482895         0.487096   \n",
       "7            0.497026           0.481703           0.480896         0.486542   \n",
       "10           0.500000           0.475366           0.482485         0.485950   \n",
       "5            0.488342           0.476933           0.482213         0.482496   \n",
       "3            0.484809           0.472010           0.463430         0.473417   \n",
       "2            0.479102           0.469151           0.463677         0.470643   \n",
       "1            0.480149           0.460980           0.457711         0.466280   \n",
       "0            0.468061           0.458920           0.467532         0.464838   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "8         0.007663                1            0.483721            0.495873   \n",
       "6         0.014220                2            0.493042            0.495743   \n",
       "11        0.009421                3            0.484266            0.493268   \n",
       "4         0.008297                4            0.488303            0.493268   \n",
       "9         0.009053                5            0.487724            0.491609   \n",
       "7         0.007421                6            0.485367            0.494063   \n",
       "10        0.010351                7            0.487002            0.492489   \n",
       "5         0.004662                8            0.478147            0.486292   \n",
       "3         0.008784                9            0.470626            0.473548   \n",
       "2         0.006385               10            0.470958            0.472421   \n",
       "1         0.009897               11            0.465159            0.472716   \n",
       "0         0.004190               12            0.463214            0.467802   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "8             0.498828          0.492807         0.006537  \n",
       "6             0.499496          0.496094         0.002647  \n",
       "11            0.496978          0.491504         0.005337  \n",
       "4             0.493333          0.491635         0.002356  \n",
       "9             0.492482          0.490605         0.002068  \n",
       "7             0.495540          0.491657         0.004488  \n",
       "10            0.495979          0.491823         0.003695  \n",
       "5             0.480782          0.481741         0.003394  \n",
       "3             0.471534          0.471903         0.001221  \n",
       "2             0.474162          0.472514         0.001310  \n",
       "1             0.468991          0.468955         0.003085  \n",
       "0             0.463576          0.464864         0.002083  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_target),\n",
    "     ('upsampler', SMOTE()),\n",
    "    ('lr', LogisticRegression(random_state=1235, solver='liblinear', max_iter=1000))]\n",
    "    )\n",
    "param = {\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'lr__C': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0],\n",
    "    }\n",
    "\n",
    "grid_lr = GridSearchCV(model, param, scoring='f1', return_train_score=True, refit=False, cv=folders)\n",
    "grid_lr.fit(features_train, target_train)\n",
    "print('Результат логистической регрессии с target_encoder')\n",
    "pd.DataFrame(grid_lr.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат логистической регрессии с ohe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_lr__C</th>\n",
       "      <th>param_lr__penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.120784</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.017778</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.505319</td>\n",
       "      <td>0.482119</td>\n",
       "      <td>0.487316</td>\n",
       "      <td>0.491585</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>1</td>\n",
       "      <td>0.481383</td>\n",
       "      <td>0.492953</td>\n",
       "      <td>0.497817</td>\n",
       "      <td>0.490718</td>\n",
       "      <td>0.006893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.125951</td>\n",
       "      <td>0.008639</td>\n",
       "      <td>0.016769</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.500655</td>\n",
       "      <td>0.486737</td>\n",
       "      <td>0.481896</td>\n",
       "      <td>0.489763</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>2</td>\n",
       "      <td>0.484789</td>\n",
       "      <td>0.501319</td>\n",
       "      <td>0.493342</td>\n",
       "      <td>0.493150</td>\n",
       "      <td>0.006750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.129673</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.017938</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.497049</td>\n",
       "      <td>0.487095</td>\n",
       "      <td>0.483678</td>\n",
       "      <td>0.489274</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>3</td>\n",
       "      <td>0.483690</td>\n",
       "      <td>0.500495</td>\n",
       "      <td>0.498661</td>\n",
       "      <td>0.494282</td>\n",
       "      <td>0.007527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.119235</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>0.021561</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>2.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 2.0, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.500329</td>\n",
       "      <td>0.482119</td>\n",
       "      <td>0.485129</td>\n",
       "      <td>0.489192</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>4</td>\n",
       "      <td>0.480211</td>\n",
       "      <td>0.497860</td>\n",
       "      <td>0.494180</td>\n",
       "      <td>0.490750</td>\n",
       "      <td>0.007602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.122638</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>0.020657</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.507641</td>\n",
       "      <td>0.474801</td>\n",
       "      <td>0.483124</td>\n",
       "      <td>0.488522</td>\n",
       "      <td>0.013940</td>\n",
       "      <td>5</td>\n",
       "      <td>0.484728</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.494502</td>\n",
       "      <td>0.493077</td>\n",
       "      <td>0.006316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.118257</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.503616</td>\n",
       "      <td>0.474667</td>\n",
       "      <td>0.486991</td>\n",
       "      <td>0.488425</td>\n",
       "      <td>0.011862</td>\n",
       "      <td>6</td>\n",
       "      <td>0.486522</td>\n",
       "      <td>0.494389</td>\n",
       "      <td>0.496473</td>\n",
       "      <td>0.492461</td>\n",
       "      <td>0.004285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.117020</td>\n",
       "      <td>0.007393</td>\n",
       "      <td>0.018023</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>2.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 2.0, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.501663</td>\n",
       "      <td>0.479788</td>\n",
       "      <td>0.483124</td>\n",
       "      <td>0.488192</td>\n",
       "      <td>0.009623</td>\n",
       "      <td>7</td>\n",
       "      <td>0.480053</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.497834</td>\n",
       "      <td>0.491252</td>\n",
       "      <td>0.007959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.115437</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.017801</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.499680</td>\n",
       "      <td>0.473718</td>\n",
       "      <td>0.491049</td>\n",
       "      <td>0.488149</td>\n",
       "      <td>0.010796</td>\n",
       "      <td>8</td>\n",
       "      <td>0.483045</td>\n",
       "      <td>0.493945</td>\n",
       "      <td>0.493590</td>\n",
       "      <td>0.490193</td>\n",
       "      <td>0.005057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.111975</td>\n",
       "      <td>0.004585</td>\n",
       "      <td>0.019577</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.504624</td>\n",
       "      <td>0.474667</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.487834</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>9</td>\n",
       "      <td>0.482600</td>\n",
       "      <td>0.495063</td>\n",
       "      <td>0.497175</td>\n",
       "      <td>0.491613</td>\n",
       "      <td>0.006431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.120042</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.018358</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.496706</td>\n",
       "      <td>0.476253</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.486986</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>10</td>\n",
       "      <td>0.485761</td>\n",
       "      <td>0.498195</td>\n",
       "      <td>0.498490</td>\n",
       "      <td>0.494149</td>\n",
       "      <td>0.005932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.110737</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.017675</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.493308</td>\n",
       "      <td>0.467384</td>\n",
       "      <td>0.469903</td>\n",
       "      <td>0.476865</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>11</td>\n",
       "      <td>0.476671</td>\n",
       "      <td>0.486383</td>\n",
       "      <td>0.485120</td>\n",
       "      <td>0.482724</td>\n",
       "      <td>0.004311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.020322</td>\n",
       "      <td>0.019491</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l1'}</td>\n",
       "      <td>0.468061</td>\n",
       "      <td>0.458920</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>0.464838</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>12</td>\n",
       "      <td>0.463214</td>\n",
       "      <td>0.467802</td>\n",
       "      <td>0.463576</td>\n",
       "      <td>0.464864</td>\n",
       "      <td>0.002083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_lr__C  \\\n",
       "5        0.120784      0.002441         0.017778        0.002456         0.1   \n",
       "8        0.125951      0.008639         0.016769        0.003438         1.0   \n",
       "4        0.129673      0.002902         0.017938        0.002192         0.1   \n",
       "10       0.119235      0.006311         0.021561        0.004059         2.0   \n",
       "6        0.122638      0.007306         0.020657        0.003419         0.5   \n",
       "9        0.118257      0.005525         0.020213        0.005752         1.0   \n",
       "11       0.117020      0.007393         0.018023        0.002249         2.0   \n",
       "2        0.115437      0.002831         0.017801        0.002533        0.01   \n",
       "3        0.111975      0.004585         0.019577        0.002222        0.01   \n",
       "7        0.120042      0.003581         0.018358        0.002291         0.5   \n",
       "1        0.110737      0.002015         0.017675        0.006356       0.001   \n",
       "0        0.121000      0.020322         0.019491        0.006529       0.001   \n",
       "\n",
       "   param_lr__penalty                                 params  \\\n",
       "5                 l2    {'lr__C': 0.1, 'lr__penalty': 'l2'}   \n",
       "8                 l1    {'lr__C': 1.0, 'lr__penalty': 'l1'}   \n",
       "4                 l1    {'lr__C': 0.1, 'lr__penalty': 'l1'}   \n",
       "10                l1    {'lr__C': 2.0, 'lr__penalty': 'l1'}   \n",
       "6                 l1    {'lr__C': 0.5, 'lr__penalty': 'l1'}   \n",
       "9                 l2    {'lr__C': 1.0, 'lr__penalty': 'l2'}   \n",
       "11                l2    {'lr__C': 2.0, 'lr__penalty': 'l2'}   \n",
       "2                 l1   {'lr__C': 0.01, 'lr__penalty': 'l1'}   \n",
       "3                 l2   {'lr__C': 0.01, 'lr__penalty': 'l2'}   \n",
       "7                 l2    {'lr__C': 0.5, 'lr__penalty': 'l2'}   \n",
       "1                 l2  {'lr__C': 0.001, 'lr__penalty': 'l2'}   \n",
       "0                 l1  {'lr__C': 0.001, 'lr__penalty': 'l1'}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "5            0.505319           0.482119           0.487316         0.491585   \n",
       "8            0.500655           0.486737           0.481896         0.489763   \n",
       "4            0.497049           0.487095           0.483678         0.489274   \n",
       "10           0.500329           0.482119           0.485129         0.489192   \n",
       "6            0.507641           0.474801           0.483124         0.488522   \n",
       "9            0.503616           0.474667           0.486991         0.488425   \n",
       "11           0.501663           0.479788           0.483124         0.488192   \n",
       "2            0.499680           0.473718           0.491049         0.488149   \n",
       "3            0.504624           0.474667           0.484211         0.487834   \n",
       "7            0.496706           0.476253           0.488000         0.486986   \n",
       "1            0.493308           0.467384           0.469903         0.476865   \n",
       "0            0.468061           0.458920           0.467532         0.464838   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "5         0.009941                1            0.481383            0.492953   \n",
       "8         0.007952                2            0.484789            0.501319   \n",
       "4         0.005672                3            0.483690            0.500495   \n",
       "10        0.007970                4            0.480211            0.497860   \n",
       "6         0.013940                5            0.484728            0.500000   \n",
       "9         0.011862                6            0.486522            0.494389   \n",
       "11        0.009623                7            0.480053            0.495868   \n",
       "2         0.010796                8            0.483045            0.493945   \n",
       "3         0.012495                9            0.482600            0.495063   \n",
       "7         0.008381               10            0.485761            0.498195   \n",
       "1         0.011672               11            0.476671            0.486383   \n",
       "0         0.004190               12            0.463214            0.467802   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "5             0.497817          0.490718         0.006893  \n",
       "8             0.493342          0.493150         0.006750  \n",
       "4             0.498661          0.494282         0.007527  \n",
       "10            0.494180          0.490750         0.007602  \n",
       "6             0.494502          0.493077         0.006316  \n",
       "9             0.496473          0.492461         0.004285  \n",
       "11            0.497834          0.491252         0.007959  \n",
       "2             0.493590          0.490193         0.005057  \n",
       "3             0.497175          0.491613         0.006431  \n",
       "7             0.498490          0.494149         0.005932  \n",
       "1             0.485120          0.482724         0.004311  \n",
       "0             0.463576          0.464864         0.002083  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "     ('upsampler', SMOTE()),\n",
    "    ('lr', LogisticRegression(random_state=1235, solver='liblinear', max_iter=1000))]\n",
    "    )\n",
    "param = {\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'lr__C': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0],\n",
    "    }\n",
    "\n",
    "grid_lr = GridSearchCV(model, param, scoring='f1', return_train_score=True, refit=False, cv=folders)\n",
    "grid_lr.fit(features_train, target_train)\n",
    "print('Результат логистической регрессии с ohe')\n",
    "pd.DataFrame(grid_lr.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получились достаточно ровные результаты с небольшим преимуществом OHE encoder. Модели на его основе и будем улучшать далее.\n",
    "Можно заметить, что для модели с OHE наилучший результат получился при минимальном предложенном значении C. Логично попробовать уменьшить этот параметр. Кроме этого, модели не переобучены, а значит можно попробовать их усложнить. Для этого попробуем перемножить признаки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат логистической регрессии с ohe и умножением признаков\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_lr__C</th>\n",
       "      <th>param_lr__penalty</th>\n",
       "      <th>param_poly__degree</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12.226996</td>\n",
       "      <td>2.971339</td>\n",
       "      <td>0.031041</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l1', 'poly__deg...</td>\n",
       "      <td>0.611821</td>\n",
       "      <td>0.594122</td>\n",
       "      <td>0.616493</td>\n",
       "      <td>0.607479</td>\n",
       "      <td>0.009635</td>\n",
       "      <td>1</td>\n",
       "      <td>0.646545</td>\n",
       "      <td>0.647339</td>\n",
       "      <td>0.642914</td>\n",
       "      <td>0.645599</td>\n",
       "      <td>0.001926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>85.698404</td>\n",
       "      <td>6.391596</td>\n",
       "      <td>0.037525</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l1', 'poly__deg...</td>\n",
       "      <td>0.608769</td>\n",
       "      <td>0.590248</td>\n",
       "      <td>0.609735</td>\n",
       "      <td>0.602917</td>\n",
       "      <td>0.008967</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658658</td>\n",
       "      <td>0.667508</td>\n",
       "      <td>0.660069</td>\n",
       "      <td>0.662078</td>\n",
       "      <td>0.003883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>55.838048</td>\n",
       "      <td>4.538914</td>\n",
       "      <td>0.040093</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l1', 'poly__deg...</td>\n",
       "      <td>0.600830</td>\n",
       "      <td>0.591453</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.602556</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>3</td>\n",
       "      <td>0.660017</td>\n",
       "      <td>0.673631</td>\n",
       "      <td>0.663016</td>\n",
       "      <td>0.665555</td>\n",
       "      <td>0.005841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.265195</td>\n",
       "      <td>0.365180</td>\n",
       "      <td>0.033481</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l2', 'poly__deg...</td>\n",
       "      <td>0.602098</td>\n",
       "      <td>0.591803</td>\n",
       "      <td>0.606742</td>\n",
       "      <td>0.600214</td>\n",
       "      <td>0.006242</td>\n",
       "      <td>4</td>\n",
       "      <td>0.653028</td>\n",
       "      <td>0.666937</td>\n",
       "      <td>0.647418</td>\n",
       "      <td>0.655794</td>\n",
       "      <td>0.008205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.923236</td>\n",
       "      <td>0.092220</td>\n",
       "      <td>0.027792</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l2', 'poly__deg...</td>\n",
       "      <td>0.603743</td>\n",
       "      <td>0.585488</td>\n",
       "      <td>0.605414</td>\n",
       "      <td>0.598215</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>5</td>\n",
       "      <td>0.647967</td>\n",
       "      <td>0.669148</td>\n",
       "      <td>0.645425</td>\n",
       "      <td>0.654180</td>\n",
       "      <td>0.010635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31.322444</td>\n",
       "      <td>0.422939</td>\n",
       "      <td>0.031176</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l1', 'poly__deg...</td>\n",
       "      <td>0.600677</td>\n",
       "      <td>0.565724</td>\n",
       "      <td>0.626346</td>\n",
       "      <td>0.597582</td>\n",
       "      <td>0.024846</td>\n",
       "      <td>6</td>\n",
       "      <td>0.602887</td>\n",
       "      <td>0.623907</td>\n",
       "      <td>0.603741</td>\n",
       "      <td>0.610178</td>\n",
       "      <td>0.009714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.195430</td>\n",
       "      <td>0.328446</td>\n",
       "      <td>0.039362</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l1', 'poly__de...</td>\n",
       "      <td>0.605706</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.601732</td>\n",
       "      <td>0.594636</td>\n",
       "      <td>0.012947</td>\n",
       "      <td>7</td>\n",
       "      <td>0.596188</td>\n",
       "      <td>0.605702</td>\n",
       "      <td>0.600291</td>\n",
       "      <td>0.600727</td>\n",
       "      <td>0.003896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.016471</td>\n",
       "      <td>0.127920</td>\n",
       "      <td>0.026156</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l2', 'poly__deg...</td>\n",
       "      <td>0.595944</td>\n",
       "      <td>0.582740</td>\n",
       "      <td>0.598437</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>8</td>\n",
       "      <td>0.636086</td>\n",
       "      <td>0.654503</td>\n",
       "      <td>0.641715</td>\n",
       "      <td>0.644101</td>\n",
       "      <td>0.007706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.773221</td>\n",
       "      <td>0.029635</td>\n",
       "      <td>0.033065</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>l2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.0005, 'lr__penalty': 'l2', 'poly__...</td>\n",
       "      <td>0.600151</td>\n",
       "      <td>0.579909</td>\n",
       "      <td>0.597037</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>9</td>\n",
       "      <td>0.601548</td>\n",
       "      <td>0.618431</td>\n",
       "      <td>0.598883</td>\n",
       "      <td>0.606287</td>\n",
       "      <td>0.008656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.716176</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>0.020291</td>\n",
       "      <td>0.006566</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l1', 'poly__deg...</td>\n",
       "      <td>0.599321</td>\n",
       "      <td>0.560996</td>\n",
       "      <td>0.613748</td>\n",
       "      <td>0.591355</td>\n",
       "      <td>0.022260</td>\n",
       "      <td>10</td>\n",
       "      <td>0.604459</td>\n",
       "      <td>0.624073</td>\n",
       "      <td>0.607827</td>\n",
       "      <td>0.612119</td>\n",
       "      <td>0.008563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.899135</td>\n",
       "      <td>0.132820</td>\n",
       "      <td>0.034023</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l2', 'poly__de...</td>\n",
       "      <td>0.596811</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.598802</td>\n",
       "      <td>0.590730</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>11</td>\n",
       "      <td>0.622468</td>\n",
       "      <td>0.633371</td>\n",
       "      <td>0.629963</td>\n",
       "      <td>0.628600</td>\n",
       "      <td>0.004554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.888207</td>\n",
       "      <td>0.011221</td>\n",
       "      <td>0.038634</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l2', 'poly__d...</td>\n",
       "      <td>0.592927</td>\n",
       "      <td>0.568862</td>\n",
       "      <td>0.609221</td>\n",
       "      <td>0.590337</td>\n",
       "      <td>0.016578</td>\n",
       "      <td>12</td>\n",
       "      <td>0.606352</td>\n",
       "      <td>0.625804</td>\n",
       "      <td>0.614572</td>\n",
       "      <td>0.615576</td>\n",
       "      <td>0.007973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.905881</td>\n",
       "      <td>0.076003</td>\n",
       "      <td>0.031377</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l2', 'poly__deg...</td>\n",
       "      <td>0.590594</td>\n",
       "      <td>0.565779</td>\n",
       "      <td>0.607225</td>\n",
       "      <td>0.587866</td>\n",
       "      <td>0.017030</td>\n",
       "      <td>13</td>\n",
       "      <td>0.590325</td>\n",
       "      <td>0.611980</td>\n",
       "      <td>0.591821</td>\n",
       "      <td>0.598042</td>\n",
       "      <td>0.009875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.571024</td>\n",
       "      <td>0.058524</td>\n",
       "      <td>0.024961</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l1', 'poly__deg...</td>\n",
       "      <td>0.590417</td>\n",
       "      <td>0.566066</td>\n",
       "      <td>0.601515</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>0.014805</td>\n",
       "      <td>14</td>\n",
       "      <td>0.589608</td>\n",
       "      <td>0.605183</td>\n",
       "      <td>0.591258</td>\n",
       "      <td>0.595350</td>\n",
       "      <td>0.006986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.973646</td>\n",
       "      <td>0.064509</td>\n",
       "      <td>0.020238</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l2', 'poly__deg...</td>\n",
       "      <td>0.583930</td>\n",
       "      <td>0.560806</td>\n",
       "      <td>0.604977</td>\n",
       "      <td>0.583237</td>\n",
       "      <td>0.018039</td>\n",
       "      <td>15</td>\n",
       "      <td>0.594824</td>\n",
       "      <td>0.611544</td>\n",
       "      <td>0.596755</td>\n",
       "      <td>0.601041</td>\n",
       "      <td>0.007469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.503514</td>\n",
       "      <td>0.046252</td>\n",
       "      <td>0.025699</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l2', 'poly__deg...</td>\n",
       "      <td>0.583147</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.590449</td>\n",
       "      <td>0.577189</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>16</td>\n",
       "      <td>0.587176</td>\n",
       "      <td>0.605079</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.593497</td>\n",
       "      <td>0.008201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.334335</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>0.025210</td>\n",
       "      <td>0.009279</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l2', 'poly__de...</td>\n",
       "      <td>0.569322</td>\n",
       "      <td>0.558739</td>\n",
       "      <td>0.589579</td>\n",
       "      <td>0.572547</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>17</td>\n",
       "      <td>0.581412</td>\n",
       "      <td>0.592324</td>\n",
       "      <td>0.575342</td>\n",
       "      <td>0.583026</td>\n",
       "      <td>0.007026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.367528</td>\n",
       "      <td>0.107638</td>\n",
       "      <td>0.020829</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l1', 'poly__de...</td>\n",
       "      <td>0.571230</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.584192</td>\n",
       "      <td>0.569756</td>\n",
       "      <td>0.012433</td>\n",
       "      <td>18</td>\n",
       "      <td>0.573112</td>\n",
       "      <td>0.580872</td>\n",
       "      <td>0.559779</td>\n",
       "      <td>0.571254</td>\n",
       "      <td>0.008711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.229111</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>0.020528</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l2', 'poly__d...</td>\n",
       "      <td>0.557303</td>\n",
       "      <td>0.548933</td>\n",
       "      <td>0.566906</td>\n",
       "      <td>0.557714</td>\n",
       "      <td>0.007343</td>\n",
       "      <td>19</td>\n",
       "      <td>0.568854</td>\n",
       "      <td>0.580074</td>\n",
       "      <td>0.556731</td>\n",
       "      <td>0.568553</td>\n",
       "      <td>0.009532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.215326</td>\n",
       "      <td>0.013503</td>\n",
       "      <td>0.021031</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.0005, 'lr__penalty': 'l2', 'poly__...</td>\n",
       "      <td>0.554974</td>\n",
       "      <td>0.542299</td>\n",
       "      <td>0.562319</td>\n",
       "      <td>0.553197</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>20</td>\n",
       "      <td>0.564740</td>\n",
       "      <td>0.576234</td>\n",
       "      <td>0.550127</td>\n",
       "      <td>0.563700</td>\n",
       "      <td>0.010684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.132181</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.021064</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l2', 'poly__deg...</td>\n",
       "      <td>0.504313</td>\n",
       "      <td>0.482072</td>\n",
       "      <td>0.484043</td>\n",
       "      <td>0.490142</td>\n",
       "      <td>0.010052</td>\n",
       "      <td>21</td>\n",
       "      <td>0.483217</td>\n",
       "      <td>0.498348</td>\n",
       "      <td>0.492792</td>\n",
       "      <td>0.491452</td>\n",
       "      <td>0.006249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.110946</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0.015616</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l1', 'poly__de...</td>\n",
       "      <td>0.498731</td>\n",
       "      <td>0.480051</td>\n",
       "      <td>0.491341</td>\n",
       "      <td>0.490041</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>22</td>\n",
       "      <td>0.483565</td>\n",
       "      <td>0.495661</td>\n",
       "      <td>0.496946</td>\n",
       "      <td>0.492057</td>\n",
       "      <td>0.006028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.119409</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.016894</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l2', 'poly__deg...</td>\n",
       "      <td>0.502994</td>\n",
       "      <td>0.479524</td>\n",
       "      <td>0.484239</td>\n",
       "      <td>0.488919</td>\n",
       "      <td>0.010137</td>\n",
       "      <td>23</td>\n",
       "      <td>0.488874</td>\n",
       "      <td>0.494567</td>\n",
       "      <td>0.496982</td>\n",
       "      <td>0.493474</td>\n",
       "      <td>0.003399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.110747</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.017829</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l2', 'poly__deg...</td>\n",
       "      <td>0.504636</td>\n",
       "      <td>0.479734</td>\n",
       "      <td>0.481848</td>\n",
       "      <td>0.488739</td>\n",
       "      <td>0.011274</td>\n",
       "      <td>24</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>0.498188</td>\n",
       "      <td>0.496676</td>\n",
       "      <td>0.492221</td>\n",
       "      <td>0.007395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.160118</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.021752</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 1.0, 'lr__penalty': 'l1', 'poly__deg...</td>\n",
       "      <td>0.497684</td>\n",
       "      <td>0.478723</td>\n",
       "      <td>0.485771</td>\n",
       "      <td>0.487393</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>25</td>\n",
       "      <td>0.480370</td>\n",
       "      <td>0.499835</td>\n",
       "      <td>0.496671</td>\n",
       "      <td>0.492292</td>\n",
       "      <td>0.008529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.128160</td>\n",
       "      <td>0.007238</td>\n",
       "      <td>0.022895</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.1, 'lr__penalty': 'l1', 'poly__deg...</td>\n",
       "      <td>0.496706</td>\n",
       "      <td>0.478667</td>\n",
       "      <td>0.485488</td>\n",
       "      <td>0.486954</td>\n",
       "      <td>0.007437</td>\n",
       "      <td>26</td>\n",
       "      <td>0.484509</td>\n",
       "      <td>0.502974</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.495661</td>\n",
       "      <td>0.008012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.123321</td>\n",
       "      <td>0.010479</td>\n",
       "      <td>0.017801</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.5, 'lr__penalty': 'l1', 'poly__deg...</td>\n",
       "      <td>0.496715</td>\n",
       "      <td>0.475683</td>\n",
       "      <td>0.482850</td>\n",
       "      <td>0.485082</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>27</td>\n",
       "      <td>0.480366</td>\n",
       "      <td>0.491136</td>\n",
       "      <td>0.496160</td>\n",
       "      <td>0.489221</td>\n",
       "      <td>0.006588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.121184</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.012592</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.01, 'lr__penalty': 'l2', 'poly__de...</td>\n",
       "      <td>0.504325</td>\n",
       "      <td>0.473232</td>\n",
       "      <td>0.477394</td>\n",
       "      <td>0.484983</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>28</td>\n",
       "      <td>0.481579</td>\n",
       "      <td>0.496719</td>\n",
       "      <td>0.495498</td>\n",
       "      <td>0.491266</td>\n",
       "      <td>0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.102559</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l2', 'poly__d...</td>\n",
       "      <td>0.495472</td>\n",
       "      <td>0.470968</td>\n",
       "      <td>0.475697</td>\n",
       "      <td>0.480712</td>\n",
       "      <td>0.010614</td>\n",
       "      <td>29</td>\n",
       "      <td>0.478108</td>\n",
       "      <td>0.488744</td>\n",
       "      <td>0.484596</td>\n",
       "      <td>0.483816</td>\n",
       "      <td>0.004377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.334642</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>0.035877</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l1', 'poly__d...</td>\n",
       "      <td>0.461150</td>\n",
       "      <td>0.475462</td>\n",
       "      <td>0.503836</td>\n",
       "      <td>0.480149</td>\n",
       "      <td>0.017739</td>\n",
       "      <td>30</td>\n",
       "      <td>0.462228</td>\n",
       "      <td>0.487050</td>\n",
       "      <td>0.494177</td>\n",
       "      <td>0.481152</td>\n",
       "      <td>0.013694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.115706</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.019388</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.0005, 'lr__penalty': 'l2', 'poly__...</td>\n",
       "      <td>0.490782</td>\n",
       "      <td>0.467630</td>\n",
       "      <td>0.465322</td>\n",
       "      <td>0.474578</td>\n",
       "      <td>0.011497</td>\n",
       "      <td>31</td>\n",
       "      <td>0.476730</td>\n",
       "      <td>0.480610</td>\n",
       "      <td>0.475591</td>\n",
       "      <td>0.477643</td>\n",
       "      <td>0.002149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.018135</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l1', 'poly__d...</td>\n",
       "      <td>0.468061</td>\n",
       "      <td>0.458920</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>0.464838</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>32</td>\n",
       "      <td>0.463214</td>\n",
       "      <td>0.467802</td>\n",
       "      <td>0.463576</td>\n",
       "      <td>0.464864</td>\n",
       "      <td>0.002083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.146408</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.023029</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.001, 'lr__penalty': 'l1', 'poly__d...</td>\n",
       "      <td>0.447776</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.446489</td>\n",
       "      <td>0.439932</td>\n",
       "      <td>0.010196</td>\n",
       "      <td>33</td>\n",
       "      <td>0.441849</td>\n",
       "      <td>0.433243</td>\n",
       "      <td>0.442499</td>\n",
       "      <td>0.439197</td>\n",
       "      <td>0.004219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146497</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>0.024244</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'lr__C': 0.0005, 'lr__penalty': 'l1', 'poly__...</td>\n",
       "      <td>0.338835</td>\n",
       "      <td>0.338318</td>\n",
       "      <td>0.338423</td>\n",
       "      <td>0.338525</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>34</td>\n",
       "      <td>0.338370</td>\n",
       "      <td>0.338629</td>\n",
       "      <td>0.338577</td>\n",
       "      <td>0.338525</td>\n",
       "      <td>0.000112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.307861</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.035450</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>l1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'lr__C': 0.0005, 'lr__penalty': 'l1', 'poly__...</td>\n",
       "      <td>0.185722</td>\n",
       "      <td>0.170306</td>\n",
       "      <td>0.182013</td>\n",
       "      <td>0.179347</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>35</td>\n",
       "      <td>0.176216</td>\n",
       "      <td>0.183865</td>\n",
       "      <td>0.178078</td>\n",
       "      <td>0.179387</td>\n",
       "      <td>0.003257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.122390</td>\n",
       "      <td>0.028118</td>\n",
       "      <td>0.025554</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'lr__C': 0.0005, 'lr__penalty': 'l1', 'poly__...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_lr__C  \\\n",
       "20      12.226996      2.971339         0.031041        0.000299         0.1   \n",
       "32      85.698404      6.391596         0.037525        0.004734         1.0   \n",
       "26      55.838048      4.538914         0.040093        0.011928         0.5   \n",
       "29       4.265195      0.365180         0.033481        0.003171         0.5   \n",
       "35       4.923236      0.092220         0.027792        0.004339         1.0   \n",
       "31      31.322444      0.422939         0.031176        0.005886         1.0   \n",
       "14       1.195430      0.328446         0.039362        0.006497        0.01   \n",
       "23       3.016471      0.127920         0.026156        0.007481         0.1   \n",
       "5        0.773221      0.029635         0.033065        0.003148      0.0005   \n",
       "25      17.716176      0.552846         0.020291        0.006566         0.5   \n",
       "17       1.899135      0.132820         0.034023        0.006336        0.01   \n",
       "11       0.888207      0.011221         0.038634        0.006403       0.001   \n",
       "28       0.905881      0.076003         0.031377        0.006109         0.5   \n",
       "19       5.571024      0.058524         0.024961        0.006870         0.1   \n",
       "34       0.973646      0.064509         0.020238        0.000030         1.0   \n",
       "22       0.503514      0.046252         0.025699        0.003860         0.1   \n",
       "16       0.334335      0.013062         0.025210        0.009279        0.01   \n",
       "13       0.367528      0.107638         0.020829        0.007356        0.01   \n",
       "10       0.229111      0.009308         0.020528        0.000298       0.001   \n",
       "4        0.215326      0.013503         0.021031        0.004092      0.0005   \n",
       "27       0.132181      0.001020         0.021064        0.003659         0.5   \n",
       "12       0.110946      0.007868         0.015616        0.000015        0.01   \n",
       "21       0.119409      0.004400         0.016894        0.003384         0.1   \n",
       "33       0.110747      0.007356         0.017829        0.003089         1.0   \n",
       "30       0.160118      0.008634         0.021752        0.000533         1.0   \n",
       "18       0.128160      0.007238         0.022895        0.002037         0.1   \n",
       "24       0.123321      0.010479         0.017801        0.003070         0.5   \n",
       "15       0.121184      0.007356         0.012592        0.004283        0.01   \n",
       "9        0.102559      0.007471         0.018896        0.000966       0.001   \n",
       "8        0.334642      0.010765         0.035877        0.003263       0.001   \n",
       "3        0.115706      0.002203         0.019388        0.002586      0.0005   \n",
       "6        0.096296      0.008303         0.018135        0.001968       0.001   \n",
       "7        0.146408      0.006356         0.023029        0.006399       0.001   \n",
       "1        0.146497      0.003870         0.024244        0.003446      0.0005   \n",
       "2        0.307861      0.009177         0.035450        0.002517      0.0005   \n",
       "0        0.122390      0.028118         0.025554        0.004157      0.0005   \n",
       "\n",
       "   param_lr__penalty param_poly__degree  \\\n",
       "20                l1                  3   \n",
       "32                l1                  3   \n",
       "26                l1                  3   \n",
       "29                l2                  3   \n",
       "35                l2                  3   \n",
       "31                l1                  2   \n",
       "14                l1                  3   \n",
       "23                l2                  3   \n",
       "5                 l2                  3   \n",
       "25                l1                  2   \n",
       "17                l2                  3   \n",
       "11                l2                  3   \n",
       "28                l2                  2   \n",
       "19                l1                  2   \n",
       "34                l2                  2   \n",
       "22                l2                  2   \n",
       "16                l2                  2   \n",
       "13                l1                  2   \n",
       "10                l2                  2   \n",
       "4                 l2                  2   \n",
       "27                l2                  1   \n",
       "12                l1                  1   \n",
       "21                l2                  1   \n",
       "33                l2                  1   \n",
       "30                l1                  1   \n",
       "18                l1                  1   \n",
       "24                l1                  1   \n",
       "15                l2                  1   \n",
       "9                 l2                  1   \n",
       "8                 l1                  3   \n",
       "3                 l2                  1   \n",
       "6                 l1                  1   \n",
       "7                 l1                  2   \n",
       "1                 l1                  2   \n",
       "2                 l1                  3   \n",
       "0                 l1                  1   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "20  {'lr__C': 0.1, 'lr__penalty': 'l1', 'poly__deg...           0.611821   \n",
       "32  {'lr__C': 1.0, 'lr__penalty': 'l1', 'poly__deg...           0.608769   \n",
       "26  {'lr__C': 0.5, 'lr__penalty': 'l1', 'poly__deg...           0.600830   \n",
       "29  {'lr__C': 0.5, 'lr__penalty': 'l2', 'poly__deg...           0.602098   \n",
       "35  {'lr__C': 1.0, 'lr__penalty': 'l2', 'poly__deg...           0.603743   \n",
       "31  {'lr__C': 1.0, 'lr__penalty': 'l1', 'poly__deg...           0.600677   \n",
       "14  {'lr__C': 0.01, 'lr__penalty': 'l1', 'poly__de...           0.605706   \n",
       "23  {'lr__C': 0.1, 'lr__penalty': 'l2', 'poly__deg...           0.595944   \n",
       "5   {'lr__C': 0.0005, 'lr__penalty': 'l2', 'poly__...           0.600151   \n",
       "25  {'lr__C': 0.5, 'lr__penalty': 'l1', 'poly__deg...           0.599321   \n",
       "17  {'lr__C': 0.01, 'lr__penalty': 'l2', 'poly__de...           0.596811   \n",
       "11  {'lr__C': 0.001, 'lr__penalty': 'l2', 'poly__d...           0.592927   \n",
       "28  {'lr__C': 0.5, 'lr__penalty': 'l2', 'poly__deg...           0.590594   \n",
       "19  {'lr__C': 0.1, 'lr__penalty': 'l1', 'poly__deg...           0.590417   \n",
       "34  {'lr__C': 1.0, 'lr__penalty': 'l2', 'poly__deg...           0.583930   \n",
       "22  {'lr__C': 0.1, 'lr__penalty': 'l2', 'poly__deg...           0.583147   \n",
       "16  {'lr__C': 0.01, 'lr__penalty': 'l2', 'poly__de...           0.569322   \n",
       "13  {'lr__C': 0.01, 'lr__penalty': 'l1', 'poly__de...           0.571230   \n",
       "10  {'lr__C': 0.001, 'lr__penalty': 'l2', 'poly__d...           0.557303   \n",
       "4   {'lr__C': 0.0005, 'lr__penalty': 'l2', 'poly__...           0.554974   \n",
       "27  {'lr__C': 0.5, 'lr__penalty': 'l2', 'poly__deg...           0.504313   \n",
       "12  {'lr__C': 0.01, 'lr__penalty': 'l1', 'poly__de...           0.498731   \n",
       "21  {'lr__C': 0.1, 'lr__penalty': 'l2', 'poly__deg...           0.502994   \n",
       "33  {'lr__C': 1.0, 'lr__penalty': 'l2', 'poly__deg...           0.504636   \n",
       "30  {'lr__C': 1.0, 'lr__penalty': 'l1', 'poly__deg...           0.497684   \n",
       "18  {'lr__C': 0.1, 'lr__penalty': 'l1', 'poly__deg...           0.496706   \n",
       "24  {'lr__C': 0.5, 'lr__penalty': 'l1', 'poly__deg...           0.496715   \n",
       "15  {'lr__C': 0.01, 'lr__penalty': 'l2', 'poly__de...           0.504325   \n",
       "9   {'lr__C': 0.001, 'lr__penalty': 'l2', 'poly__d...           0.495472   \n",
       "8   {'lr__C': 0.001, 'lr__penalty': 'l1', 'poly__d...           0.461150   \n",
       "3   {'lr__C': 0.0005, 'lr__penalty': 'l2', 'poly__...           0.490782   \n",
       "6   {'lr__C': 0.001, 'lr__penalty': 'l1', 'poly__d...           0.468061   \n",
       "7   {'lr__C': 0.001, 'lr__penalty': 'l1', 'poly__d...           0.447776   \n",
       "1   {'lr__C': 0.0005, 'lr__penalty': 'l1', 'poly__...           0.338835   \n",
       "2   {'lr__C': 0.0005, 'lr__penalty': 'l1', 'poly__...           0.185722   \n",
       "0   {'lr__C': 0.0005, 'lr__penalty': 'l1', 'poly__...           0.000000   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "20           0.594122           0.616493         0.607479        0.009635   \n",
       "32           0.590248           0.609735         0.602917        0.008967   \n",
       "26           0.591453           0.615385         0.602556        0.009846   \n",
       "29           0.591803           0.606742         0.600214        0.006242   \n",
       "35           0.585488           0.605414         0.598215        0.009025   \n",
       "31           0.565724           0.626346         0.597582        0.024846   \n",
       "14           0.576471           0.601732         0.594636        0.012947   \n",
       "23           0.582740           0.598437         0.592374        0.006888   \n",
       "5            0.579909           0.597037         0.592366        0.008900   \n",
       "25           0.560996           0.613748         0.591355        0.022260   \n",
       "17           0.576577           0.598802         0.590730        0.010041   \n",
       "11           0.568862           0.609221         0.590337        0.016578   \n",
       "28           0.565779           0.607225         0.587866        0.017030   \n",
       "19           0.566066           0.601515         0.586000        0.014805   \n",
       "34           0.560806           0.604977         0.583237        0.018039   \n",
       "22           0.557971           0.590449         0.577189        0.013912   \n",
       "16           0.558739           0.589579         0.572547        0.012795   \n",
       "13           0.553846           0.584192         0.569756        0.012433   \n",
       "10           0.548933           0.566906         0.557714        0.007343   \n",
       "4            0.542299           0.562319         0.553197        0.008269   \n",
       "27           0.482072           0.484043         0.490142        0.010052   \n",
       "12           0.480051           0.491341         0.490041        0.007681   \n",
       "21           0.479524           0.484239         0.488919        0.010137   \n",
       "33           0.479734           0.481848         0.488739        0.011274   \n",
       "30           0.478723           0.485771         0.487393        0.007825   \n",
       "18           0.478667           0.485488         0.486954        0.007437   \n",
       "24           0.475683           0.482850         0.485082        0.008730   \n",
       "15           0.473232           0.477394         0.484983        0.013781   \n",
       "9            0.470968           0.475697         0.480712        0.010614   \n",
       "8            0.475462           0.503836         0.480149        0.017739   \n",
       "3            0.467630           0.465322         0.474578        0.011497   \n",
       "6            0.458920           0.467532         0.464838        0.004190   \n",
       "7            0.425532           0.446489         0.439932        0.010196   \n",
       "1            0.338318           0.338423         0.338525        0.000223   \n",
       "2            0.170306           0.182013         0.179347        0.006570   \n",
       "0            0.000000           0.000000         0.000000        0.000000   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "20                1            0.646545            0.647339   \n",
       "32                2            0.658658            0.667508   \n",
       "26                3            0.660017            0.673631   \n",
       "29                4            0.653028            0.666937   \n",
       "35                5            0.647967            0.669148   \n",
       "31                6            0.602887            0.623907   \n",
       "14                7            0.596188            0.605702   \n",
       "23                8            0.636086            0.654503   \n",
       "5                 9            0.601548            0.618431   \n",
       "25               10            0.604459            0.624073   \n",
       "17               11            0.622468            0.633371   \n",
       "11               12            0.606352            0.625804   \n",
       "28               13            0.590325            0.611980   \n",
       "19               14            0.589608            0.605183   \n",
       "34               15            0.594824            0.611544   \n",
       "22               16            0.587176            0.605079   \n",
       "16               17            0.581412            0.592324   \n",
       "13               18            0.573112            0.580872   \n",
       "10               19            0.568854            0.580074   \n",
       "4                20            0.564740            0.576234   \n",
       "27               21            0.483217            0.498348   \n",
       "12               22            0.483565            0.495661   \n",
       "21               23            0.488874            0.494567   \n",
       "33               24            0.481800            0.498188   \n",
       "30               25            0.480370            0.499835   \n",
       "18               26            0.484509            0.502974   \n",
       "24               27            0.480366            0.491136   \n",
       "15               28            0.481579            0.496719   \n",
       "9                29            0.478108            0.488744   \n",
       "8                30            0.462228            0.487050   \n",
       "3                31            0.476730            0.480610   \n",
       "6                32            0.463214            0.467802   \n",
       "7                33            0.441849            0.433243   \n",
       "1                34            0.338370            0.338629   \n",
       "2                35            0.176216            0.183865   \n",
       "0                36            0.000000            0.000000   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "20            0.642914          0.645599         0.001926  \n",
       "32            0.660069          0.662078         0.003883  \n",
       "26            0.663016          0.665555         0.005841  \n",
       "29            0.647418          0.655794         0.008205  \n",
       "35            0.645425          0.654180         0.010635  \n",
       "31            0.603741          0.610178         0.009714  \n",
       "14            0.600291          0.600727         0.003896  \n",
       "23            0.641715          0.644101         0.007706  \n",
       "5             0.598883          0.606287         0.008656  \n",
       "25            0.607827          0.612119         0.008563  \n",
       "17            0.629963          0.628600         0.004554  \n",
       "11            0.614572          0.615576         0.007973  \n",
       "28            0.591821          0.598042         0.009875  \n",
       "19            0.591258          0.595350         0.006986  \n",
       "34            0.596755          0.601041         0.007469  \n",
       "22            0.588235          0.593497         0.008201  \n",
       "16            0.575342          0.583026         0.007026  \n",
       "13            0.559779          0.571254         0.008711  \n",
       "10            0.556731          0.568553         0.009532  \n",
       "4             0.550127          0.563700         0.010684  \n",
       "27            0.492792          0.491452         0.006249  \n",
       "12            0.496946          0.492057         0.006028  \n",
       "21            0.496982          0.493474         0.003399  \n",
       "33            0.496676          0.492221         0.007395  \n",
       "30            0.496671          0.492292         0.008529  \n",
       "18            0.499500          0.495661         0.008012  \n",
       "24            0.496160          0.489221         0.006588  \n",
       "15            0.495498          0.491266         0.006868  \n",
       "9             0.484596          0.483816         0.004377  \n",
       "8             0.494177          0.481152         0.013694  \n",
       "3             0.475591          0.477643         0.002149  \n",
       "6             0.463576          0.464864         0.002083  \n",
       "7             0.442499          0.439197         0.004219  \n",
       "1             0.338577          0.338525         0.000112  \n",
       "2             0.178078          0.179387         0.003257  \n",
       "0             0.000000          0.000000         0.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('upsampler', SMOTE()),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('lr', LogisticRegression(random_state=1235, solver='liblinear', max_iter=1000))]\n",
    "    )\n",
    "\n",
    "param = {\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'lr__C': [0.0005, 0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "    'poly__degree': [1, 2, 3]\n",
    "    }\n",
    "\n",
    "grid_lr = GridSearchCV(model, param, scoring='f1', return_train_score=True, refit=False, cv=folders)\n",
    "grid_lr.fit(features_train, target_train)\n",
    "print('Результат логистической регрессии с ohe и умножением признаков')\n",
    "pd.DataFrame(grid_lr.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам удалось найти гиперпараметры, заметно улучшившие показатель качества. Теперь появилось переобучение, но оно относительно невелико. На этих значениях остановимся и перейдем к работе со случайным лесом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат случайного леса с target_encoder\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.394772</td>\n",
       "      <td>0.060462</td>\n",
       "      <td>0.157713</td>\n",
       "      <td>0.027480</td>\n",
       "      <td>16</td>\n",
       "      <td>90</td>\n",
       "      <td>{'rf__max_depth': 16, 'rf__n_estimators': 90}</td>\n",
       "      <td>0.602851</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.603622</td>\n",
       "      <td>0.600074</td>\n",
       "      <td>0.004483</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.989862</td>\n",
       "      <td>0.994929</td>\n",
       "      <td>0.992780</td>\n",
       "      <td>0.002139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.844422</td>\n",
       "      <td>0.043584</td>\n",
       "      <td>0.066703</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>{'rf__max_depth': 8, 'rf__n_estimators': 30}</td>\n",
       "      <td>0.604869</td>\n",
       "      <td>0.581016</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.598685</td>\n",
       "      <td>0.012680</td>\n",
       "      <td>2</td>\n",
       "      <td>0.681711</td>\n",
       "      <td>0.676136</td>\n",
       "      <td>0.686481</td>\n",
       "      <td>0.681443</td>\n",
       "      <td>0.004228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.346783</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.033780</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>{'rf__max_depth': 8, 'rf__n_estimators': 10}</td>\n",
       "      <td>0.608531</td>\n",
       "      <td>0.577281</td>\n",
       "      <td>0.605833</td>\n",
       "      <td>0.597215</td>\n",
       "      <td>0.014138</td>\n",
       "      <td>3</td>\n",
       "      <td>0.686994</td>\n",
       "      <td>0.680948</td>\n",
       "      <td>0.665056</td>\n",
       "      <td>0.677666</td>\n",
       "      <td>0.009252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.970333</td>\n",
       "      <td>0.124324</td>\n",
       "      <td>0.107985</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>{'rf__max_depth': 8, 'rf__n_estimators': 90}</td>\n",
       "      <td>0.600948</td>\n",
       "      <td>0.588348</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>0.597189</td>\n",
       "      <td>0.006275</td>\n",
       "      <td>4</td>\n",
       "      <td>0.687947</td>\n",
       "      <td>0.686736</td>\n",
       "      <td>0.682785</td>\n",
       "      <td>0.685823</td>\n",
       "      <td>0.002204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.913274</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.073792</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>{'rf__max_depth': 16, 'rf__n_estimators': 30}</td>\n",
       "      <td>0.601842</td>\n",
       "      <td>0.569388</td>\n",
       "      <td>0.600595</td>\n",
       "      <td>0.590608</td>\n",
       "      <td>0.015014</td>\n",
       "      <td>5</td>\n",
       "      <td>0.983789</td>\n",
       "      <td>0.986162</td>\n",
       "      <td>0.985628</td>\n",
       "      <td>0.985193</td>\n",
       "      <td>0.001017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.374080</td>\n",
       "      <td>0.024329</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>{'rf__max_depth': 4, 'rf__n_estimators': 90}</td>\n",
       "      <td>0.569132</td>\n",
       "      <td>0.559603</td>\n",
       "      <td>0.589831</td>\n",
       "      <td>0.572855</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>6</td>\n",
       "      <td>0.586306</td>\n",
       "      <td>0.608163</td>\n",
       "      <td>0.588335</td>\n",
       "      <td>0.594268</td>\n",
       "      <td>0.009860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.665903</td>\n",
       "      <td>0.085180</td>\n",
       "      <td>0.045411</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>{'rf__max_depth': 4, 'rf__n_estimators': 30}</td>\n",
       "      <td>0.586151</td>\n",
       "      <td>0.561035</td>\n",
       "      <td>0.569748</td>\n",
       "      <td>0.572311</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>7</td>\n",
       "      <td>0.596761</td>\n",
       "      <td>0.595267</td>\n",
       "      <td>0.591211</td>\n",
       "      <td>0.594413</td>\n",
       "      <td>0.002345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.290292</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.034145</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>{'rf__max_depth': 4, 'rf__n_estimators': 10}</td>\n",
       "      <td>0.563403</td>\n",
       "      <td>0.549763</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.556240</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>8</td>\n",
       "      <td>0.580016</td>\n",
       "      <td>0.585803</td>\n",
       "      <td>0.567308</td>\n",
       "      <td>0.577709</td>\n",
       "      <td>0.007725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.401692</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>0.040536</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>{'rf__max_depth': 16, 'rf__n_estimators': 10}</td>\n",
       "      <td>0.557477</td>\n",
       "      <td>0.543750</td>\n",
       "      <td>0.566567</td>\n",
       "      <td>0.555931</td>\n",
       "      <td>0.009379</td>\n",
       "      <td>9</td>\n",
       "      <td>0.958662</td>\n",
       "      <td>0.965323</td>\n",
       "      <td>0.967412</td>\n",
       "      <td>0.963799</td>\n",
       "      <td>0.003731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.022068</td>\n",
       "      <td>0.008473</td>\n",
       "      <td>0.081263</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>{'rf__max_depth': 2, 'rf__n_estimators': 90}</td>\n",
       "      <td>0.543462</td>\n",
       "      <td>0.521230</td>\n",
       "      <td>0.511734</td>\n",
       "      <td>0.525475</td>\n",
       "      <td>0.013297</td>\n",
       "      <td>10</td>\n",
       "      <td>0.543340</td>\n",
       "      <td>0.541143</td>\n",
       "      <td>0.518104</td>\n",
       "      <td>0.534195</td>\n",
       "      <td>0.011414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.277234</td>\n",
       "      <td>0.045550</td>\n",
       "      <td>0.032213</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'rf__max_depth': 2, 'rf__n_estimators': 10}</td>\n",
       "      <td>0.521614</td>\n",
       "      <td>0.500371</td>\n",
       "      <td>0.488798</td>\n",
       "      <td>0.503594</td>\n",
       "      <td>0.013589</td>\n",
       "      <td>11</td>\n",
       "      <td>0.525380</td>\n",
       "      <td>0.511459</td>\n",
       "      <td>0.506986</td>\n",
       "      <td>0.514608</td>\n",
       "      <td>0.007832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.444345</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.046395</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>{'rf__max_depth': 2, 'rf__n_estimators': 30}</td>\n",
       "      <td>0.481032</td>\n",
       "      <td>0.497784</td>\n",
       "      <td>0.493344</td>\n",
       "      <td>0.490720</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>12</td>\n",
       "      <td>0.489198</td>\n",
       "      <td>0.495620</td>\n",
       "      <td>0.499805</td>\n",
       "      <td>0.494874</td>\n",
       "      <td>0.004363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "11       2.394772      0.060462         0.157713        0.027480   \n",
       "7        0.844422      0.043584         0.066703        0.006731   \n",
       "6        0.346783      0.008517         0.033780        0.002556   \n",
       "8        1.970333      0.124324         0.107985        0.002175   \n",
       "10       0.913274      0.007041         0.073792        0.010193   \n",
       "5        1.374080      0.024329         0.088867        0.002509   \n",
       "4        0.665903      0.085180         0.045411        0.002129   \n",
       "3        0.290292      0.001929         0.034145        0.002573   \n",
       "9        0.401692      0.005212         0.040536        0.005205   \n",
       "2        1.022068      0.008473         0.081263        0.001954   \n",
       "0        0.277234      0.045550         0.032213        0.004402   \n",
       "1        0.444345      0.006381         0.046395        0.002580   \n",
       "\n",
       "   param_rf__max_depth param_rf__n_estimators  \\\n",
       "11                  16                     90   \n",
       "7                    8                     30   \n",
       "6                    8                     10   \n",
       "8                    8                     90   \n",
       "10                  16                     30   \n",
       "5                    4                     90   \n",
       "4                    4                     30   \n",
       "3                    4                     10   \n",
       "9                   16                     10   \n",
       "2                    2                     90   \n",
       "0                    2                     10   \n",
       "1                    2                     30   \n",
       "\n",
       "                                           params  split0_test_score  \\\n",
       "11  {'rf__max_depth': 16, 'rf__n_estimators': 90}           0.602851   \n",
       "7    {'rf__max_depth': 8, 'rf__n_estimators': 30}           0.604869   \n",
       "6    {'rf__max_depth': 8, 'rf__n_estimators': 10}           0.608531   \n",
       "8    {'rf__max_depth': 8, 'rf__n_estimators': 90}           0.600948   \n",
       "10  {'rf__max_depth': 16, 'rf__n_estimators': 30}           0.601842   \n",
       "5    {'rf__max_depth': 4, 'rf__n_estimators': 90}           0.569132   \n",
       "4    {'rf__max_depth': 4, 'rf__n_estimators': 30}           0.586151   \n",
       "3    {'rf__max_depth': 4, 'rf__n_estimators': 10}           0.563403   \n",
       "9   {'rf__max_depth': 16, 'rf__n_estimators': 10}           0.557477   \n",
       "2    {'rf__max_depth': 2, 'rf__n_estimators': 90}           0.543462   \n",
       "0    {'rf__max_depth': 2, 'rf__n_estimators': 10}           0.521614   \n",
       "1    {'rf__max_depth': 2, 'rf__n_estimators': 30}           0.481032   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "11           0.593750           0.603622         0.600074        0.004483   \n",
       "7            0.581016           0.610169         0.598685        0.012680   \n",
       "6            0.577281           0.605833         0.597215        0.014138   \n",
       "8            0.588348           0.602273         0.597189        0.006275   \n",
       "10           0.569388           0.600595         0.590608        0.015014   \n",
       "5            0.559603           0.589831         0.572855        0.012618   \n",
       "4            0.561035           0.569748         0.572311        0.010413   \n",
       "3            0.549763           0.555556         0.556240        0.005589   \n",
       "9            0.543750           0.566567         0.555931        0.009379   \n",
       "2            0.521230           0.511734         0.525475        0.013297   \n",
       "0            0.500371           0.488798         0.503594        0.013589   \n",
       "1            0.497784           0.493344         0.490720        0.007086   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "11                1            0.993548            0.989862   \n",
       "7                 2            0.681711            0.676136   \n",
       "6                 3            0.686994            0.680948   \n",
       "8                 4            0.687947            0.686736   \n",
       "10                5            0.983789            0.986162   \n",
       "5                 6            0.586306            0.608163   \n",
       "4                 7            0.596761            0.595267   \n",
       "3                 8            0.580016            0.585803   \n",
       "9                 9            0.958662            0.965323   \n",
       "2                10            0.543340            0.541143   \n",
       "0                11            0.525380            0.511459   \n",
       "1                12            0.489198            0.495620   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "11            0.994929          0.992780         0.002139  \n",
       "7             0.686481          0.681443         0.004228  \n",
       "6             0.665056          0.677666         0.009252  \n",
       "8             0.682785          0.685823         0.002204  \n",
       "10            0.985628          0.985193         0.001017  \n",
       "5             0.588335          0.594268         0.009860  \n",
       "4             0.591211          0.594413         0.002345  \n",
       "3             0.567308          0.577709         0.007725  \n",
       "9             0.967412          0.963799         0.003731  \n",
       "2             0.518104          0.534195         0.011414  \n",
       "0             0.506986          0.514608         0.007832  \n",
       "1             0.499805          0.494874         0.004363  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_target),\n",
    "    ('upsampler', SMOTE()),\n",
    "    ('rf', RandomForestClassifier(random_state=1235))]\n",
    "    )\n",
    "param = {\n",
    "    'rf__n_estimators': [10, 30, 90],\n",
    "    'rf__max_depth': [2, 4, 8, 16],\n",
    "    }\n",
    "\n",
    "grid_rf = GridSearchCV(model, param, scoring='f1', return_train_score=True, refit=False, cv=folders)\n",
    "grid_rf.fit(features_train, target_train)\n",
    "print('Результат случайного леса с target_encoder')\n",
    "pd.DataFrame(grid_rf.cv_results_).sort_values(by='rank_test_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат случайного леса с ohe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.922305</td>\n",
       "      <td>0.059796</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.017206</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>{'rf__max_depth': 8, 'rf__n_estimators': 90}</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.597761</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.614208</td>\n",
       "      <td>0.011631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693895</td>\n",
       "      <td>0.682533</td>\n",
       "      <td>0.681443</td>\n",
       "      <td>0.685957</td>\n",
       "      <td>0.005631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.729868</td>\n",
       "      <td>0.105490</td>\n",
       "      <td>0.045969</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>{'rf__max_depth': 8, 'rf__n_estimators': 30}</td>\n",
       "      <td>0.613946</td>\n",
       "      <td>0.595055</td>\n",
       "      <td>0.613424</td>\n",
       "      <td>0.607475</td>\n",
       "      <td>0.008785</td>\n",
       "      <td>2</td>\n",
       "      <td>0.690848</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.683283</td>\n",
       "      <td>0.687210</td>\n",
       "      <td>0.003095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.588950</td>\n",
       "      <td>0.173946</td>\n",
       "      <td>0.138916</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>16</td>\n",
       "      <td>90</td>\n",
       "      <td>{'rf__max_depth': 16, 'rf__n_estimators': 90}</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.606544</td>\n",
       "      <td>0.016020</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987238</td>\n",
       "      <td>0.989061</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.988437</td>\n",
       "      <td>0.000848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.280831</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.032293</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>{'rf__max_depth': 8, 'rf__n_estimators': 10}</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.580372</td>\n",
       "      <td>0.620865</td>\n",
       "      <td>0.604116</td>\n",
       "      <td>0.017255</td>\n",
       "      <td>4</td>\n",
       "      <td>0.676730</td>\n",
       "      <td>0.660025</td>\n",
       "      <td>0.667244</td>\n",
       "      <td>0.667999</td>\n",
       "      <td>0.006841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.976740</td>\n",
       "      <td>0.057703</td>\n",
       "      <td>0.068319</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>{'rf__max_depth': 16, 'rf__n_estimators': 30}</td>\n",
       "      <td>0.614218</td>\n",
       "      <td>0.589028</td>\n",
       "      <td>0.595057</td>\n",
       "      <td>0.599434</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>5</td>\n",
       "      <td>0.981253</td>\n",
       "      <td>0.980874</td>\n",
       "      <td>0.980752</td>\n",
       "      <td>0.980960</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.278265</td>\n",
       "      <td>0.041901</td>\n",
       "      <td>0.081097</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>{'rf__max_depth': 4, 'rf__n_estimators': 90}</td>\n",
       "      <td>0.595107</td>\n",
       "      <td>0.574319</td>\n",
       "      <td>0.598291</td>\n",
       "      <td>0.589239</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>6</td>\n",
       "      <td>0.606544</td>\n",
       "      <td>0.603846</td>\n",
       "      <td>0.587601</td>\n",
       "      <td>0.599331</td>\n",
       "      <td>0.008367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.352204</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.032056</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>{'rf__max_depth': 16, 'rf__n_estimators': 10}</td>\n",
       "      <td>0.589942</td>\n",
       "      <td>0.579310</td>\n",
       "      <td>0.596849</td>\n",
       "      <td>0.588700</td>\n",
       "      <td>0.007214</td>\n",
       "      <td>7</td>\n",
       "      <td>0.958390</td>\n",
       "      <td>0.958390</td>\n",
       "      <td>0.956960</td>\n",
       "      <td>0.957914</td>\n",
       "      <td>0.000674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.586037</td>\n",
       "      <td>0.070067</td>\n",
       "      <td>0.052720</td>\n",
       "      <td>0.008153</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>{'rf__max_depth': 4, 'rf__n_estimators': 30}</td>\n",
       "      <td>0.577010</td>\n",
       "      <td>0.572823</td>\n",
       "      <td>0.595107</td>\n",
       "      <td>0.581647</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>8</td>\n",
       "      <td>0.598485</td>\n",
       "      <td>0.599602</td>\n",
       "      <td>0.593762</td>\n",
       "      <td>0.597283</td>\n",
       "      <td>0.002531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.215477</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>0.028042</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>{'rf__max_depth': 4, 'rf__n_estimators': 10}</td>\n",
       "      <td>0.576037</td>\n",
       "      <td>0.560500</td>\n",
       "      <td>0.597647</td>\n",
       "      <td>0.578061</td>\n",
       "      <td>0.015233</td>\n",
       "      <td>9</td>\n",
       "      <td>0.587867</td>\n",
       "      <td>0.586636</td>\n",
       "      <td>0.583399</td>\n",
       "      <td>0.585967</td>\n",
       "      <td>0.001884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.019201</td>\n",
       "      <td>0.049453</td>\n",
       "      <td>0.077161</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>{'rf__max_depth': 2, 'rf__n_estimators': 90}</td>\n",
       "      <td>0.557257</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.572694</td>\n",
       "      <td>0.563014</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>10</td>\n",
       "      <td>0.566444</td>\n",
       "      <td>0.573338</td>\n",
       "      <td>0.554261</td>\n",
       "      <td>0.564681</td>\n",
       "      <td>0.007888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373473</td>\n",
       "      <td>0.007662</td>\n",
       "      <td>0.037261</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>{'rf__max_depth': 2, 'rf__n_estimators': 30}</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.527520</td>\n",
       "      <td>0.575624</td>\n",
       "      <td>0.548667</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>11</td>\n",
       "      <td>0.547886</td>\n",
       "      <td>0.546406</td>\n",
       "      <td>0.555596</td>\n",
       "      <td>0.549963</td>\n",
       "      <td>0.004029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.196759</td>\n",
       "      <td>0.017358</td>\n",
       "      <td>0.028078</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'rf__max_depth': 2, 'rf__n_estimators': 10}</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.531609</td>\n",
       "      <td>0.534441</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>12</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.562013</td>\n",
       "      <td>0.522848</td>\n",
       "      <td>0.539398</td>\n",
       "      <td>0.016554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "8        1.922305      0.059796         0.113569        0.017206   \n",
       "7        0.729868      0.105490         0.045969        0.002958   \n",
       "11       2.588950      0.173946         0.138916        0.011623   \n",
       "6        0.280831      0.002546         0.032293        0.000059   \n",
       "10       0.976740      0.057703         0.068319        0.001603   \n",
       "5        1.278265      0.041901         0.081097        0.001922   \n",
       "9        0.352204      0.004215         0.032056        0.000180   \n",
       "4        0.586037      0.070067         0.052720        0.008153   \n",
       "3        0.215477      0.002515         0.028042        0.002609   \n",
       "2        1.019201      0.049453         0.077161        0.003229   \n",
       "1        0.373473      0.007662         0.037261        0.000105   \n",
       "0        0.196759      0.017358         0.028078        0.002267   \n",
       "\n",
       "   param_rf__max_depth param_rf__n_estimators  \\\n",
       "8                    8                     90   \n",
       "7                    8                     30   \n",
       "11                  16                     90   \n",
       "6                    8                     10   \n",
       "10                  16                     30   \n",
       "5                    4                     90   \n",
       "9                   16                     10   \n",
       "4                    4                     30   \n",
       "3                    4                     10   \n",
       "2                    2                     90   \n",
       "1                    2                     30   \n",
       "0                    2                     10   \n",
       "\n",
       "                                           params  split0_test_score  \\\n",
       "8    {'rf__max_depth': 8, 'rf__n_estimators': 90}           0.622222   \n",
       "7    {'rf__max_depth': 8, 'rf__n_estimators': 30}           0.613946   \n",
       "11  {'rf__max_depth': 16, 'rf__n_estimators': 90}           0.625000   \n",
       "6    {'rf__max_depth': 8, 'rf__n_estimators': 10}           0.611111   \n",
       "10  {'rf__max_depth': 16, 'rf__n_estimators': 30}           0.614218   \n",
       "5    {'rf__max_depth': 4, 'rf__n_estimators': 90}           0.595107   \n",
       "9   {'rf__max_depth': 16, 'rf__n_estimators': 10}           0.589942   \n",
       "4    {'rf__max_depth': 4, 'rf__n_estimators': 30}           0.577010   \n",
       "3    {'rf__max_depth': 4, 'rf__n_estimators': 10}           0.576037   \n",
       "2    {'rf__max_depth': 2, 'rf__n_estimators': 90}           0.557257   \n",
       "1    {'rf__max_depth': 2, 'rf__n_estimators': 30}           0.542857   \n",
       "0    {'rf__max_depth': 2, 'rf__n_estimators': 10}           0.536000   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "8            0.597761           0.622642         0.614208        0.011631   \n",
       "7            0.595055           0.613424         0.607475        0.008785   \n",
       "11           0.585938           0.608696         0.606544        0.016020   \n",
       "6            0.580372           0.620865         0.604116        0.017255   \n",
       "10           0.589028           0.595057         0.599434        0.010740   \n",
       "5            0.574319           0.598291         0.589239        0.010630   \n",
       "9            0.579310           0.596849         0.588700        0.007214   \n",
       "4            0.572823           0.595107         0.581647        0.009670   \n",
       "3            0.560500           0.597647         0.578061        0.015233   \n",
       "2            0.559091           0.572694         0.563014        0.006885   \n",
       "1            0.527520           0.575624         0.548667        0.020064   \n",
       "0            0.535714           0.531609         0.534441        0.002006   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "8                 1            0.693895            0.682533   \n",
       "7                 2            0.690848            0.687500   \n",
       "11                3            0.987238            0.989061   \n",
       "6                 4            0.676730            0.660025   \n",
       "10                5            0.981253            0.980874   \n",
       "5                 6            0.606544            0.603846   \n",
       "9                 7            0.958390            0.958390   \n",
       "4                 8            0.598485            0.599602   \n",
       "3                 9            0.587867            0.586636   \n",
       "2                10            0.566444            0.573338   \n",
       "1                11            0.547886            0.546406   \n",
       "0                12            0.533333            0.562013   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "8             0.681443          0.685957         0.005631  \n",
       "7             0.683283          0.687210         0.003095  \n",
       "11            0.989011          0.988437         0.000848  \n",
       "6             0.667244          0.667999         0.006841  \n",
       "10            0.980752          0.980960         0.000213  \n",
       "5             0.587601          0.599331         0.008367  \n",
       "9             0.956960          0.957914         0.000674  \n",
       "4             0.593762          0.597283         0.002531  \n",
       "3             0.583399          0.585967         0.001884  \n",
       "2             0.554261          0.564681         0.007888  \n",
       "1             0.555596          0.549963         0.004029  \n",
       "0             0.522848          0.539398         0.016554  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('upsampler', SMOTE()),\n",
    "    ('rf', RandomForestClassifier(random_state=1235))]\n",
    "    )\n",
    "param = {\n",
    "    'rf__n_estimators': [10, 30, 90],\n",
    "    'rf__max_depth': [2, 4, 8, 16],\n",
    "    }\n",
    "\n",
    "grid_rf = GridSearchCV(model, param, scoring='f1', return_train_score=True, refit=False, cv=folders)\n",
    "grid_rf.fit(features_train, target_train)\n",
    "print('Результат случайного леса с ohe')\n",
    "pd.DataFrame(grid_rf.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и с линейной регрессией лучше себя показал onehotencoder. Также заметно, что модели с глубиной шестнадцать показывают неплохую метрику на валидационной выборке, но высокая степень переобучения не позволяет понять, как они поведут себя на новых данных. Также мы видим, что рост числа деревьев дает относительно небольшой прирост в точности. Попробуем тщательней поискать подходящие гиперпараметры в районе глубины 4-10 и до 60 оценщиков. Также ранее мы видели, что логистическая регрессия с пользой использовала перемножение признаков. Применим этот подход и здесь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат случайного леса с ohe и перемножением признаков\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_poly__degree</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.262001</td>\n",
       "      <td>0.032137</td>\n",
       "      <td>0.080746</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 10, 'rf__...</td>\n",
       "      <td>0.618667</td>\n",
       "      <td>0.612932</td>\n",
       "      <td>0.628011</td>\n",
       "      <td>0.619870</td>\n",
       "      <td>0.006214</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758072</td>\n",
       "      <td>0.755048</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.758854</td>\n",
       "      <td>0.003471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.872420</td>\n",
       "      <td>0.076990</td>\n",
       "      <td>0.081832</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 8, 'rf__n...</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.600340</td>\n",
       "      <td>0.631399</td>\n",
       "      <td>0.617651</td>\n",
       "      <td>0.012928</td>\n",
       "      <td>2</td>\n",
       "      <td>0.712212</td>\n",
       "      <td>0.714774</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.715245</td>\n",
       "      <td>0.002690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.479909</td>\n",
       "      <td>0.080945</td>\n",
       "      <td>0.097524</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 10, 'rf__...</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>0.614711</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>0.615355</td>\n",
       "      <td>0.006285</td>\n",
       "      <td>3</td>\n",
       "      <td>0.785080</td>\n",
       "      <td>0.789427</td>\n",
       "      <td>0.791229</td>\n",
       "      <td>0.788578</td>\n",
       "      <td>0.002581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.307612</td>\n",
       "      <td>0.218353</td>\n",
       "      <td>0.091028</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 10, 'rf__...</td>\n",
       "      <td>0.612818</td>\n",
       "      <td>0.603611</td>\n",
       "      <td>0.627887</td>\n",
       "      <td>0.614772</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>4</td>\n",
       "      <td>0.806678</td>\n",
       "      <td>0.814456</td>\n",
       "      <td>0.810284</td>\n",
       "      <td>0.810473</td>\n",
       "      <td>0.003178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.174713</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.042613</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 10, 'rf__...</td>\n",
       "      <td>0.622807</td>\n",
       "      <td>0.600518</td>\n",
       "      <td>0.620508</td>\n",
       "      <td>0.614611</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>5</td>\n",
       "      <td>0.773940</td>\n",
       "      <td>0.774137</td>\n",
       "      <td>0.771300</td>\n",
       "      <td>0.773126</td>\n",
       "      <td>0.001293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.607305</td>\n",
       "      <td>0.127898</td>\n",
       "      <td>0.086796</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 8, 'rf__n...</td>\n",
       "      <td>0.618061</td>\n",
       "      <td>0.595432</td>\n",
       "      <td>0.623201</td>\n",
       "      <td>0.612231</td>\n",
       "      <td>0.012063</td>\n",
       "      <td>6</td>\n",
       "      <td>0.733783</td>\n",
       "      <td>0.723226</td>\n",
       "      <td>0.742882</td>\n",
       "      <td>0.733297</td>\n",
       "      <td>0.008032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.321762</td>\n",
       "      <td>0.034569</td>\n",
       "      <td>0.083951</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 10, 'rf__...</td>\n",
       "      <td>0.608853</td>\n",
       "      <td>0.600707</td>\n",
       "      <td>0.627046</td>\n",
       "      <td>0.612202</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>7</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>0.776940</td>\n",
       "      <td>0.792301</td>\n",
       "      <td>0.787920</td>\n",
       "      <td>0.007817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.289381</td>\n",
       "      <td>0.308905</td>\n",
       "      <td>0.073437</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 6, 'rf__n...</td>\n",
       "      <td>0.620347</td>\n",
       "      <td>0.591319</td>\n",
       "      <td>0.623960</td>\n",
       "      <td>0.611875</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>8</td>\n",
       "      <td>0.654757</td>\n",
       "      <td>0.649044</td>\n",
       "      <td>0.649537</td>\n",
       "      <td>0.651113</td>\n",
       "      <td>0.002585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.029905</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>0.062888</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 8, 'rf__n...</td>\n",
       "      <td>0.612389</td>\n",
       "      <td>0.600993</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.611532</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>9</td>\n",
       "      <td>0.711954</td>\n",
       "      <td>0.711950</td>\n",
       "      <td>0.713973</td>\n",
       "      <td>0.712626</td>\n",
       "      <td>0.000953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.946401</td>\n",
       "      <td>0.037884</td>\n",
       "      <td>0.070880</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 10, 'rf__...</td>\n",
       "      <td>0.619893</td>\n",
       "      <td>0.597198</td>\n",
       "      <td>0.617262</td>\n",
       "      <td>0.611451</td>\n",
       "      <td>0.010136</td>\n",
       "      <td>10</td>\n",
       "      <td>0.772807</td>\n",
       "      <td>0.754271</td>\n",
       "      <td>0.763239</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.007569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.270876</td>\n",
       "      <td>0.013027</td>\n",
       "      <td>0.056890</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 8, 'rf__n...</td>\n",
       "      <td>0.614996</td>\n",
       "      <td>0.595570</td>\n",
       "      <td>0.622517</td>\n",
       "      <td>0.611027</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>11</td>\n",
       "      <td>0.727808</td>\n",
       "      <td>0.727663</td>\n",
       "      <td>0.743701</td>\n",
       "      <td>0.733058</td>\n",
       "      <td>0.007526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6.891761</td>\n",
       "      <td>0.205657</td>\n",
       "      <td>0.115485</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 8, 'rf__n...</td>\n",
       "      <td>0.615643</td>\n",
       "      <td>0.597338</td>\n",
       "      <td>0.618454</td>\n",
       "      <td>0.610478</td>\n",
       "      <td>0.009362</td>\n",
       "      <td>12</td>\n",
       "      <td>0.725820</td>\n",
       "      <td>0.727973</td>\n",
       "      <td>0.735809</td>\n",
       "      <td>0.729867</td>\n",
       "      <td>0.004292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.722275</td>\n",
       "      <td>0.053578</td>\n",
       "      <td>0.056132</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 6, 'rf__n...</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.624894</td>\n",
       "      <td>0.610476</td>\n",
       "      <td>0.012413</td>\n",
       "      <td>13</td>\n",
       "      <td>0.647864</td>\n",
       "      <td>0.649918</td>\n",
       "      <td>0.648417</td>\n",
       "      <td>0.648733</td>\n",
       "      <td>0.000868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>7.970495</td>\n",
       "      <td>0.260697</td>\n",
       "      <td>0.121533</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 10, 'rf__...</td>\n",
       "      <td>0.618538</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.622414</td>\n",
       "      <td>0.610232</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>14</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.808979</td>\n",
       "      <td>0.828789</td>\n",
       "      <td>0.818556</td>\n",
       "      <td>0.008101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.745640</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.067431</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 10, 'rf__...</td>\n",
       "      <td>0.615929</td>\n",
       "      <td>0.588037</td>\n",
       "      <td>0.619792</td>\n",
       "      <td>0.607919</td>\n",
       "      <td>0.014147</td>\n",
       "      <td>15</td>\n",
       "      <td>0.808063</td>\n",
       "      <td>0.802092</td>\n",
       "      <td>0.804628</td>\n",
       "      <td>0.804928</td>\n",
       "      <td>0.002447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.864004</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.062033</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 8, 'rf__n...</td>\n",
       "      <td>0.614859</td>\n",
       "      <td>0.588946</td>\n",
       "      <td>0.618842</td>\n",
       "      <td>0.607549</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>16</td>\n",
       "      <td>0.685121</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.683633</td>\n",
       "      <td>0.684352</td>\n",
       "      <td>0.000609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.873846</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.058676</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 6, 'rf__n...</td>\n",
       "      <td>0.616776</td>\n",
       "      <td>0.589198</td>\n",
       "      <td>0.615261</td>\n",
       "      <td>0.607078</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>17</td>\n",
       "      <td>0.663366</td>\n",
       "      <td>0.655724</td>\n",
       "      <td>0.653924</td>\n",
       "      <td>0.657672</td>\n",
       "      <td>0.004093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.155222</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.069728</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 8, 'rf__n...</td>\n",
       "      <td>0.609735</td>\n",
       "      <td>0.597116</td>\n",
       "      <td>0.613811</td>\n",
       "      <td>0.606887</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>18</td>\n",
       "      <td>0.681067</td>\n",
       "      <td>0.681683</td>\n",
       "      <td>0.678923</td>\n",
       "      <td>0.680558</td>\n",
       "      <td>0.001183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.173144</td>\n",
       "      <td>0.054437</td>\n",
       "      <td>0.052610</td>\n",
       "      <td>0.008725</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 8, 'rf__n...</td>\n",
       "      <td>0.621735</td>\n",
       "      <td>0.585204</td>\n",
       "      <td>0.609966</td>\n",
       "      <td>0.605635</td>\n",
       "      <td>0.015225</td>\n",
       "      <td>19</td>\n",
       "      <td>0.700546</td>\n",
       "      <td>0.700791</td>\n",
       "      <td>0.717430</td>\n",
       "      <td>0.706255</td>\n",
       "      <td>0.007902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.480011</td>\n",
       "      <td>0.015576</td>\n",
       "      <td>0.042651</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 8, 'rf__n...</td>\n",
       "      <td>0.618644</td>\n",
       "      <td>0.593033</td>\n",
       "      <td>0.605197</td>\n",
       "      <td>0.605625</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>20</td>\n",
       "      <td>0.686644</td>\n",
       "      <td>0.679882</td>\n",
       "      <td>0.669231</td>\n",
       "      <td>0.678586</td>\n",
       "      <td>0.007168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.307875</td>\n",
       "      <td>0.041581</td>\n",
       "      <td>0.099511</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 6, 'rf__n...</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.587854</td>\n",
       "      <td>0.614122</td>\n",
       "      <td>0.604863</td>\n",
       "      <td>0.012043</td>\n",
       "      <td>21</td>\n",
       "      <td>0.662024</td>\n",
       "      <td>0.658248</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.660134</td>\n",
       "      <td>0.001541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.473109</td>\n",
       "      <td>0.449994</td>\n",
       "      <td>0.078036</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 6, 'rf__n...</td>\n",
       "      <td>0.614379</td>\n",
       "      <td>0.591195</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.604757</td>\n",
       "      <td>0.009866</td>\n",
       "      <td>22</td>\n",
       "      <td>0.667756</td>\n",
       "      <td>0.651311</td>\n",
       "      <td>0.658678</td>\n",
       "      <td>0.659248</td>\n",
       "      <td>0.006726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.890670</td>\n",
       "      <td>0.015741</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 6, 'rf__n...</td>\n",
       "      <td>0.602871</td>\n",
       "      <td>0.600830</td>\n",
       "      <td>0.609121</td>\n",
       "      <td>0.604274</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>23</td>\n",
       "      <td>0.640288</td>\n",
       "      <td>0.652588</td>\n",
       "      <td>0.637453</td>\n",
       "      <td>0.643443</td>\n",
       "      <td>0.006569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.506922</td>\n",
       "      <td>0.038066</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.011714</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 10, 'rf__...</td>\n",
       "      <td>0.605521</td>\n",
       "      <td>0.603509</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.603010</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>24</td>\n",
       "      <td>0.754784</td>\n",
       "      <td>0.744634</td>\n",
       "      <td>0.742122</td>\n",
       "      <td>0.747180</td>\n",
       "      <td>0.005474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.428576</td>\n",
       "      <td>0.028139</td>\n",
       "      <td>0.032272</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 6, 'rf__n...</td>\n",
       "      <td>0.609325</td>\n",
       "      <td>0.591935</td>\n",
       "      <td>0.601467</td>\n",
       "      <td>0.600909</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>25</td>\n",
       "      <td>0.628641</td>\n",
       "      <td>0.626369</td>\n",
       "      <td>0.614751</td>\n",
       "      <td>0.623254</td>\n",
       "      <td>0.006084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.762556</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>0.058606</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 6, 'rf__n...</td>\n",
       "      <td>0.604305</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.598102</td>\n",
       "      <td>0.010487</td>\n",
       "      <td>26</td>\n",
       "      <td>0.631623</td>\n",
       "      <td>0.636808</td>\n",
       "      <td>0.621008</td>\n",
       "      <td>0.629813</td>\n",
       "      <td>0.006576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.115986</td>\n",
       "      <td>0.022003</td>\n",
       "      <td>0.080358</td>\n",
       "      <td>0.006987</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 6, 'rf__n...</td>\n",
       "      <td>0.592352</td>\n",
       "      <td>0.590191</td>\n",
       "      <td>0.607055</td>\n",
       "      <td>0.596533</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>27</td>\n",
       "      <td>0.634347</td>\n",
       "      <td>0.639439</td>\n",
       "      <td>0.624334</td>\n",
       "      <td>0.632707</td>\n",
       "      <td>0.006275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.869008</td>\n",
       "      <td>0.129768</td>\n",
       "      <td>0.064262</td>\n",
       "      <td>0.003457</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 4, 'rf__n...</td>\n",
       "      <td>0.598716</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.605505</td>\n",
       "      <td>0.593073</td>\n",
       "      <td>0.013077</td>\n",
       "      <td>28</td>\n",
       "      <td>0.605242</td>\n",
       "      <td>0.607466</td>\n",
       "      <td>0.599463</td>\n",
       "      <td>0.604057</td>\n",
       "      <td>0.003373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.645617</td>\n",
       "      <td>0.027650</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 4, 'rf__n...</td>\n",
       "      <td>0.583015</td>\n",
       "      <td>0.582884</td>\n",
       "      <td>0.606777</td>\n",
       "      <td>0.590892</td>\n",
       "      <td>0.011233</td>\n",
       "      <td>29</td>\n",
       "      <td>0.596719</td>\n",
       "      <td>0.611710</td>\n",
       "      <td>0.589449</td>\n",
       "      <td>0.599293</td>\n",
       "      <td>0.009269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.194885</td>\n",
       "      <td>0.099511</td>\n",
       "      <td>0.095125</td>\n",
       "      <td>0.006906</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 4, 'rf__n...</td>\n",
       "      <td>0.591806</td>\n",
       "      <td>0.573171</td>\n",
       "      <td>0.599693</td>\n",
       "      <td>0.588223</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>30</td>\n",
       "      <td>0.609626</td>\n",
       "      <td>0.608298</td>\n",
       "      <td>0.602530</td>\n",
       "      <td>0.606818</td>\n",
       "      <td>0.003080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.232017</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.053147</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 4, 'rf__n...</td>\n",
       "      <td>0.593622</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.601399</td>\n",
       "      <td>0.587943</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>31</td>\n",
       "      <td>0.599101</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>0.588652</td>\n",
       "      <td>0.597551</td>\n",
       "      <td>0.006723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.667902</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>0.075322</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 4, 'rf__n...</td>\n",
       "      <td>0.582353</td>\n",
       "      <td>0.576351</td>\n",
       "      <td>0.596947</td>\n",
       "      <td>0.585217</td>\n",
       "      <td>0.008649</td>\n",
       "      <td>32</td>\n",
       "      <td>0.594277</td>\n",
       "      <td>0.612292</td>\n",
       "      <td>0.606526</td>\n",
       "      <td>0.604365</td>\n",
       "      <td>0.007512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.403984</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.055262</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 4, 'rf__n...</td>\n",
       "      <td>0.585843</td>\n",
       "      <td>0.574335</td>\n",
       "      <td>0.592824</td>\n",
       "      <td>0.584334</td>\n",
       "      <td>0.007623</td>\n",
       "      <td>33</td>\n",
       "      <td>0.601292</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.605750</td>\n",
       "      <td>0.606051</td>\n",
       "      <td>0.004014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.648712</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.051515</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 4, 'rf__n...</td>\n",
       "      <td>0.585635</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.588329</td>\n",
       "      <td>0.583810</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>34</td>\n",
       "      <td>0.592681</td>\n",
       "      <td>0.591049</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>0.590577</td>\n",
       "      <td>0.001940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.953458</td>\n",
       "      <td>0.057631</td>\n",
       "      <td>0.072487</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 4, 'rf__n...</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.566719</td>\n",
       "      <td>0.592075</td>\n",
       "      <td>0.582647</td>\n",
       "      <td>0.011326</td>\n",
       "      <td>35</td>\n",
       "      <td>0.600078</td>\n",
       "      <td>0.593907</td>\n",
       "      <td>0.588781</td>\n",
       "      <td>0.594255</td>\n",
       "      <td>0.004618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.369928</td>\n",
       "      <td>0.016096</td>\n",
       "      <td>0.036468</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 4, 'rf__n...</td>\n",
       "      <td>0.574351</td>\n",
       "      <td>0.578017</td>\n",
       "      <td>0.586478</td>\n",
       "      <td>0.579615</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>36</td>\n",
       "      <td>0.602236</td>\n",
       "      <td>0.594697</td>\n",
       "      <td>0.588647</td>\n",
       "      <td>0.595193</td>\n",
       "      <td>0.005559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.458562</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.037367</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 2, 'rf__n...</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.570288</td>\n",
       "      <td>0.554106</td>\n",
       "      <td>0.564272</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>37</td>\n",
       "      <td>0.558330</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.556420</td>\n",
       "      <td>0.568409</td>\n",
       "      <td>0.015623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.137024</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.062134</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 2, 'rf__n...</td>\n",
       "      <td>0.556041</td>\n",
       "      <td>0.553441</td>\n",
       "      <td>0.575517</td>\n",
       "      <td>0.561666</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>38</td>\n",
       "      <td>0.561674</td>\n",
       "      <td>0.577633</td>\n",
       "      <td>0.570508</td>\n",
       "      <td>0.569938</td>\n",
       "      <td>0.006528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.795795</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.045869</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 2, 'rf__max_depth': 2, 'rf__n...</td>\n",
       "      <td>0.553459</td>\n",
       "      <td>0.560250</td>\n",
       "      <td>0.561056</td>\n",
       "      <td>0.558255</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>39</td>\n",
       "      <td>0.559552</td>\n",
       "      <td>0.576744</td>\n",
       "      <td>0.555092</td>\n",
       "      <td>0.563796</td>\n",
       "      <td>0.009335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.059328</td>\n",
       "      <td>0.165037</td>\n",
       "      <td>0.061396</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 2, 'rf__n...</td>\n",
       "      <td>0.566345</td>\n",
       "      <td>0.545185</td>\n",
       "      <td>0.543085</td>\n",
       "      <td>0.551538</td>\n",
       "      <td>0.010505</td>\n",
       "      <td>40</td>\n",
       "      <td>0.549165</td>\n",
       "      <td>0.553019</td>\n",
       "      <td>0.547443</td>\n",
       "      <td>0.549876</td>\n",
       "      <td>0.002331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.470103</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.069354</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 2, 'rf__n...</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.540460</td>\n",
       "      <td>0.550974</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>41</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>0.564228</td>\n",
       "      <td>0.542001</td>\n",
       "      <td>0.557776</td>\n",
       "      <td>0.011216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.693509</td>\n",
       "      <td>0.008862</td>\n",
       "      <td>0.065746</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 2, 'rf__n...</td>\n",
       "      <td>0.543433</td>\n",
       "      <td>0.551625</td>\n",
       "      <td>0.535613</td>\n",
       "      <td>0.543557</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>42</td>\n",
       "      <td>0.559155</td>\n",
       "      <td>0.569675</td>\n",
       "      <td>0.535815</td>\n",
       "      <td>0.554882</td>\n",
       "      <td>0.014150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.274581</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.033952</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>{'poly__degree': 1, 'rf__max_depth': 2, 'rf__n...</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.550417</td>\n",
       "      <td>0.521803</td>\n",
       "      <td>0.542974</td>\n",
       "      <td>0.015188</td>\n",
       "      <td>43</td>\n",
       "      <td>0.569607</td>\n",
       "      <td>0.571534</td>\n",
       "      <td>0.528962</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.019630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.693846</td>\n",
       "      <td>0.085179</td>\n",
       "      <td>0.068044</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 2, 'rf__n...</td>\n",
       "      <td>0.541259</td>\n",
       "      <td>0.533043</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.535878</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>44</td>\n",
       "      <td>0.537991</td>\n",
       "      <td>0.541202</td>\n",
       "      <td>0.528622</td>\n",
       "      <td>0.535938</td>\n",
       "      <td>0.005337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.327200</td>\n",
       "      <td>0.043777</td>\n",
       "      <td>0.080113</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>{'poly__degree': 3, 'rf__max_depth': 2, 'rf__n...</td>\n",
       "      <td>0.546207</td>\n",
       "      <td>0.510316</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.530918</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>45</td>\n",
       "      <td>0.543779</td>\n",
       "      <td>0.520755</td>\n",
       "      <td>0.534232</td>\n",
       "      <td>0.532922</td>\n",
       "      <td>0.009445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "14       1.262001      0.032137         0.080746        0.002815   \n",
       "26       2.872420      0.076990         0.081832        0.004555   \n",
       "29       3.479909      0.080945         0.097524        0.003100   \n",
       "43       5.307612      0.218353         0.091028        0.005282   \n",
       "27       1.174713      0.011151         0.042613        0.002726   \n",
       "40       4.607305      0.127898         0.086796        0.002605   \n",
       "28       2.321762      0.034569         0.083951        0.017562   \n",
       "23       2.289381      0.308905         0.073437        0.002679   \n",
       "25       2.029905      0.047593         0.062888        0.001375   \n",
       "13       0.946401      0.037884         0.070880        0.006571   \n",
       "39       2.270876      0.013027         0.056890        0.006411   \n",
       "41       6.891761      0.205657         0.115485        0.006114   \n",
       "22       1.722275      0.053578         0.056132        0.002453   \n",
       "44       7.970495      0.260697         0.121533        0.004403   \n",
       "42       2.745640      0.042400         0.067431        0.002362   \n",
       "10       0.864004      0.020264         0.062033        0.002827   \n",
       "36       1.873846      0.005195         0.058676        0.000543   \n",
       "11       1.155222      0.019518         0.069728        0.003705   \n",
       "24       1.173144      0.054437         0.052610        0.008725   \n",
       "9        0.480011      0.015576         0.042651        0.000140   \n",
       "38       5.307875      0.041581         0.099511        0.002406   \n",
       "37       3.473109      0.449994         0.078036        0.011194   \n",
       "21       0.890670      0.015741         0.040782        0.002929   \n",
       "12       0.506922      0.038066         0.045539        0.011714   \n",
       "6        0.428576      0.028139         0.032272        0.004403   \n",
       "7        0.762556      0.008919         0.058606        0.008046   \n",
       "8        1.115986      0.022003         0.080358        0.006987   \n",
       "20       1.869008      0.129768         0.064262        0.003457   \n",
       "18       0.645617      0.027650         0.037319        0.001525   \n",
       "35       4.194885      0.099511         0.095125        0.006906   \n",
       "19       1.232017      0.000436         0.053147        0.000245   \n",
       "34       2.667902      0.020627         0.075322        0.005108   \n",
       "33       1.403984      0.013896         0.055262        0.002575   \n",
       "4        0.648712      0.011832         0.051515        0.003062   \n",
       "5        0.953458      0.057631         0.072487        0.002089   \n",
       "3        0.369928      0.016096         0.036468        0.007368   \n",
       "15       0.458562      0.007229         0.037367        0.004146   \n",
       "17       1.137024      0.007758         0.062134        0.002336   \n",
       "16       0.795795      0.004637         0.045869        0.002438   \n",
       "30       1.059328      0.165037         0.061396        0.005638   \n",
       "1        0.470103      0.027211         0.069354        0.023008   \n",
       "2        0.693509      0.008862         0.065746        0.004767   \n",
       "0        0.274581      0.026390         0.033952        0.004792   \n",
       "31       1.693846      0.085179         0.068044        0.003400   \n",
       "32       2.327200      0.043777         0.080113        0.004281   \n",
       "\n",
       "   param_poly__degree param_rf__max_depth param_rf__n_estimators  \\\n",
       "14                  1                  10                     60   \n",
       "26                  2                   8                     60   \n",
       "29                  2                  10                     60   \n",
       "43                  3                  10                     40   \n",
       "27                  2                  10                     20   \n",
       "40                  3                   8                     40   \n",
       "28                  2                  10                     40   \n",
       "23                  2                   6                     60   \n",
       "25                  2                   8                     40   \n",
       "13                  1                  10                     40   \n",
       "39                  3                   8                     20   \n",
       "41                  3                   8                     60   \n",
       "22                  2                   6                     40   \n",
       "44                  3                  10                     60   \n",
       "42                  3                  10                     20   \n",
       "10                  1                   8                     40   \n",
       "36                  3                   6                     20   \n",
       "11                  1                   8                     60   \n",
       "24                  2                   8                     20   \n",
       "9                   1                   8                     20   \n",
       "38                  3                   6                     60   \n",
       "37                  3                   6                     40   \n",
       "21                  2                   6                     20   \n",
       "12                  1                  10                     20   \n",
       "6                   1                   6                     20   \n",
       "7                   1                   6                     40   \n",
       "8                   1                   6                     60   \n",
       "20                  2                   4                     60   \n",
       "18                  2                   4                     20   \n",
       "35                  3                   4                     60   \n",
       "19                  2                   4                     40   \n",
       "34                  3                   4                     40   \n",
       "33                  3                   4                     20   \n",
       "4                   1                   4                     40   \n",
       "5                   1                   4                     60   \n",
       "3                   1                   4                     20   \n",
       "15                  2                   2                     20   \n",
       "17                  2                   2                     60   \n",
       "16                  2                   2                     40   \n",
       "30                  3                   2                     20   \n",
       "1                   1                   2                     40   \n",
       "2                   1                   2                     60   \n",
       "0                   1                   2                     20   \n",
       "31                  3                   2                     40   \n",
       "32                  3                   2                     60   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "14  {'poly__degree': 1, 'rf__max_depth': 10, 'rf__...           0.618667   \n",
       "26  {'poly__degree': 2, 'rf__max_depth': 8, 'rf__n...           0.621212   \n",
       "29  {'poly__degree': 2, 'rf__max_depth': 10, 'rf__...           0.608000   \n",
       "43  {'poly__degree': 3, 'rf__max_depth': 10, 'rf__...           0.612818   \n",
       "27  {'poly__degree': 2, 'rf__max_depth': 10, 'rf__...           0.622807   \n",
       "40  {'poly__degree': 3, 'rf__max_depth': 8, 'rf__n...           0.618061   \n",
       "28  {'poly__degree': 2, 'rf__max_depth': 10, 'rf__...           0.608853   \n",
       "23  {'poly__degree': 2, 'rf__max_depth': 6, 'rf__n...           0.620347   \n",
       "25  {'poly__degree': 2, 'rf__max_depth': 8, 'rf__n...           0.612389   \n",
       "13  {'poly__degree': 1, 'rf__max_depth': 10, 'rf__...           0.619893   \n",
       "39  {'poly__degree': 3, 'rf__max_depth': 8, 'rf__n...           0.614996   \n",
       "41  {'poly__degree': 3, 'rf__max_depth': 8, 'rf__n...           0.615643   \n",
       "22  {'poly__degree': 2, 'rf__max_depth': 6, 'rf__n...           0.611940   \n",
       "44  {'poly__degree': 3, 'rf__max_depth': 10, 'rf__...           0.618538   \n",
       "42  {'poly__degree': 3, 'rf__max_depth': 10, 'rf__...           0.615929   \n",
       "10  {'poly__degree': 1, 'rf__max_depth': 8, 'rf__n...           0.614859   \n",
       "36  {'poly__degree': 3, 'rf__max_depth': 6, 'rf__n...           0.616776   \n",
       "11  {'poly__degree': 1, 'rf__max_depth': 8, 'rf__n...           0.609735   \n",
       "24  {'poly__degree': 2, 'rf__max_depth': 8, 'rf__n...           0.621735   \n",
       "9   {'poly__degree': 1, 'rf__max_depth': 8, 'rf__n...           0.618644   \n",
       "38  {'poly__degree': 3, 'rf__max_depth': 6, 'rf__n...           0.612613   \n",
       "37  {'poly__degree': 3, 'rf__max_depth': 6, 'rf__n...           0.614379   \n",
       "21  {'poly__degree': 2, 'rf__max_depth': 6, 'rf__n...           0.602871   \n",
       "12  {'poly__degree': 1, 'rf__max_depth': 10, 'rf__...           0.605521   \n",
       "6   {'poly__degree': 1, 'rf__max_depth': 6, 'rf__n...           0.609325   \n",
       "7   {'poly__degree': 1, 'rf__max_depth': 6, 'rf__n...           0.604305   \n",
       "8   {'poly__degree': 1, 'rf__max_depth': 6, 'rf__n...           0.592352   \n",
       "20  {'poly__degree': 2, 'rf__max_depth': 4, 'rf__n...           0.598716   \n",
       "18  {'poly__degree': 2, 'rf__max_depth': 4, 'rf__n...           0.583015   \n",
       "35  {'poly__degree': 3, 'rf__max_depth': 4, 'rf__n...           0.591806   \n",
       "19  {'poly__degree': 2, 'rf__max_depth': 4, 'rf__n...           0.593622   \n",
       "34  {'poly__degree': 3, 'rf__max_depth': 4, 'rf__n...           0.582353   \n",
       "33  {'poly__degree': 3, 'rf__max_depth': 4, 'rf__n...           0.585843   \n",
       "4   {'poly__degree': 1, 'rf__max_depth': 4, 'rf__n...           0.585635   \n",
       "5   {'poly__degree': 1, 'rf__max_depth': 4, 'rf__n...           0.589147   \n",
       "3   {'poly__degree': 1, 'rf__max_depth': 4, 'rf__n...           0.574351   \n",
       "15  {'poly__degree': 2, 'rf__max_depth': 2, 'rf__n...           0.568421   \n",
       "17  {'poly__degree': 2, 'rf__max_depth': 2, 'rf__n...           0.556041   \n",
       "16  {'poly__degree': 2, 'rf__max_depth': 2, 'rf__n...           0.553459   \n",
       "30  {'poly__degree': 3, 'rf__max_depth': 2, 'rf__n...           0.566345   \n",
       "1   {'poly__degree': 1, 'rf__max_depth': 2, 'rf__n...           0.558007   \n",
       "2   {'poly__degree': 1, 'rf__max_depth': 2, 'rf__n...           0.543433   \n",
       "0   {'poly__degree': 1, 'rf__max_depth': 2, 'rf__n...           0.556701   \n",
       "31  {'poly__degree': 3, 'rf__max_depth': 2, 'rf__n...           0.541259   \n",
       "32  {'poly__degree': 3, 'rf__max_depth': 2, 'rf__n...           0.546207   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "14           0.612932           0.628011         0.619870        0.006214   \n",
       "26           0.600340           0.631399         0.617651        0.012928   \n",
       "29           0.614711           0.623354         0.615355        0.006285   \n",
       "43           0.603611           0.627887         0.614772        0.010006   \n",
       "27           0.600518           0.620508         0.614611        0.010010   \n",
       "40           0.595432           0.623201         0.612231        0.012063   \n",
       "28           0.600707           0.627046         0.612202        0.011010   \n",
       "23           0.591319           0.623960         0.611875        0.014610   \n",
       "25           0.600993           0.621212         0.611532        0.008277   \n",
       "13           0.597198           0.617262         0.611451        0.010136   \n",
       "39           0.595570           0.622517         0.611027        0.011353   \n",
       "41           0.597338           0.618454         0.610478        0.009362   \n",
       "22           0.594595           0.624894         0.610476        0.012413   \n",
       "44           0.589744           0.622414         0.610232        0.014574   \n",
       "42           0.588037           0.619792         0.607919        0.014147   \n",
       "10           0.588946           0.618842         0.607549        0.013254   \n",
       "36           0.589198           0.615261         0.607078        0.012658   \n",
       "11           0.597116           0.613811         0.606887        0.007107   \n",
       "24           0.585204           0.609966         0.605635        0.015225   \n",
       "9            0.593033           0.605197         0.605625        0.010460   \n",
       "38           0.587854           0.614122         0.604863        0.012043   \n",
       "37           0.591195           0.608696         0.604757        0.009866   \n",
       "21           0.600830           0.609121         0.604274        0.003527   \n",
       "12           0.603509           0.600000         0.603010        0.002281   \n",
       "6            0.591935           0.601467         0.600909        0.007110   \n",
       "7            0.583333           0.606667         0.598102        0.010487   \n",
       "8            0.590191           0.607055         0.596533        0.007493   \n",
       "20           0.575000           0.605505         0.593073        0.013077   \n",
       "18           0.582884           0.606777         0.590892        0.011233   \n",
       "35           0.573171           0.599693         0.588223        0.011120   \n",
       "19           0.568807           0.601399         0.587943        0.013898   \n",
       "34           0.576351           0.596947         0.585217        0.008649   \n",
       "33           0.574335           0.592824         0.584334        0.007623   \n",
       "4            0.577465           0.588329         0.583810        0.004619   \n",
       "5            0.566719           0.592075         0.582647        0.011326   \n",
       "3            0.578017           0.586478         0.579615        0.005078   \n",
       "15           0.570288           0.554106         0.564272        0.007228   \n",
       "17           0.553441           0.575517         0.561666        0.009851   \n",
       "16           0.560250           0.561056         0.558255        0.003407   \n",
       "30           0.545185           0.543085         0.551538        0.010505   \n",
       "1            0.554455           0.540460         0.550974        0.007575   \n",
       "2            0.551625           0.535613         0.543557        0.006537   \n",
       "0            0.550417           0.521803         0.542974        0.015188   \n",
       "31           0.533043           0.533333         0.535878        0.003806   \n",
       "32           0.510316           0.536232         0.530918        0.015126   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "14                1            0.758072            0.755048   \n",
       "26                2            0.712212            0.714774   \n",
       "29                3            0.785080            0.789427   \n",
       "43                4            0.806678            0.814456   \n",
       "27                5            0.773940            0.774137   \n",
       "40                6            0.733783            0.723226   \n",
       "28                7            0.794521            0.776940   \n",
       "23                8            0.654757            0.649044   \n",
       "25                9            0.711954            0.711950   \n",
       "13               10            0.772807            0.754271   \n",
       "39               11            0.727808            0.727663   \n",
       "41               12            0.725820            0.727973   \n",
       "22               13            0.647864            0.649918   \n",
       "44               14            0.817900            0.808979   \n",
       "42               15            0.808063            0.802092   \n",
       "10               16            0.685121            0.684300   \n",
       "36               17            0.663366            0.655724   \n",
       "11               18            0.681067            0.681683   \n",
       "24               19            0.700546            0.700791   \n",
       "9                20            0.686644            0.679882   \n",
       "38               21            0.662024            0.658248   \n",
       "37               22            0.667756            0.651311   \n",
       "21               23            0.640288            0.652588   \n",
       "12               24            0.754784            0.744634   \n",
       "6                25            0.628641            0.626369   \n",
       "7                26            0.631623            0.636808   \n",
       "8                27            0.634347            0.639439   \n",
       "20               28            0.605242            0.607466   \n",
       "18               29            0.596719            0.611710   \n",
       "35               30            0.609626            0.608298   \n",
       "19               31            0.599101            0.604900   \n",
       "34               32            0.594277            0.612292   \n",
       "33               33            0.601292            0.611111   \n",
       "4                34            0.592681            0.591049   \n",
       "5                35            0.600078            0.593907   \n",
       "3                36            0.602236            0.594697   \n",
       "15               37            0.558330            0.590476   \n",
       "17               38            0.561674            0.577633   \n",
       "16               39            0.559552            0.576744   \n",
       "30               40            0.549165            0.553019   \n",
       "1                41            0.567100            0.564228   \n",
       "2                42            0.559155            0.569675   \n",
       "0                43            0.569607            0.571534   \n",
       "31               44            0.537991            0.541202   \n",
       "32               45            0.543779            0.520755   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "14            0.763441          0.758854         0.003471  \n",
       "26            0.718750          0.715245         0.002690  \n",
       "29            0.791229          0.788578         0.002581  \n",
       "43            0.810284          0.810473         0.003178  \n",
       "27            0.771300          0.773126         0.001293  \n",
       "40            0.742882          0.733297         0.008032  \n",
       "28            0.792301          0.787920         0.007817  \n",
       "23            0.649537          0.651113         0.002585  \n",
       "25            0.713973          0.712626         0.000953  \n",
       "13            0.763239          0.763439         0.007569  \n",
       "39            0.743701          0.733058         0.007526  \n",
       "41            0.735809          0.729867         0.004292  \n",
       "22            0.648417          0.648733         0.000868  \n",
       "44            0.828789          0.818556         0.008101  \n",
       "42            0.804628          0.804928         0.002447  \n",
       "10            0.683633          0.684352         0.000609  \n",
       "36            0.653924          0.657672         0.004093  \n",
       "11            0.678923          0.680558         0.001183  \n",
       "24            0.717430          0.706255         0.007902  \n",
       "9             0.669231          0.678586         0.007168  \n",
       "38            0.660131          0.660134         0.001541  \n",
       "37            0.658678          0.659248         0.006726  \n",
       "21            0.637453          0.643443         0.006569  \n",
       "12            0.742122          0.747180         0.005474  \n",
       "6             0.614751          0.623254         0.006084  \n",
       "7             0.621008          0.629813         0.006576  \n",
       "8             0.624334          0.632707         0.006275  \n",
       "20            0.599463          0.604057         0.003373  \n",
       "18            0.589449          0.599293         0.009269  \n",
       "35            0.602530          0.606818         0.003080  \n",
       "19            0.588652          0.597551         0.006723  \n",
       "34            0.606526          0.604365         0.007512  \n",
       "33            0.605750          0.606051         0.004014  \n",
       "4             0.588000          0.590577         0.001940  \n",
       "5             0.588781          0.594255         0.004618  \n",
       "3             0.588647          0.595193         0.005559  \n",
       "15            0.556420          0.568409         0.015623  \n",
       "17            0.570508          0.569938         0.006528  \n",
       "16            0.555092          0.563796         0.009335  \n",
       "30            0.547443          0.549876         0.002331  \n",
       "1             0.542001          0.557776         0.011216  \n",
       "2             0.535815          0.554882         0.014150  \n",
       "0             0.528962          0.556701         0.019630  \n",
       "31            0.528622          0.535938         0.005337  \n",
       "32            0.534232          0.532922         0.009445  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('upsampler', SMOTE()),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('rf', RandomForestClassifier(random_state=1235))]\n",
    "    )\n",
    "param = {\n",
    "    'rf__n_estimators': [20, 40, 60],\n",
    "    'rf__max_depth': [2, 4, 6, 8, 10],\n",
    "    'poly__degree': [1, 2, 3]\n",
    "    }\n",
    "\n",
    "grid_rf = GridSearchCV(model, param, scoring='f1', return_train_score=True, refit=False, cv=folders)\n",
    "grid_rf.fit(features_train, target_train)\n",
    "print('Результат случайного леса с ohe и перемножением признаков')\n",
    "pd.DataFrame(grid_rf.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перспективной выглядит модель с глубиной шесть, 60 оценщиками и второй степенью при умножении признаков. Она лишь чуть-чуть хуже лучших образцов, но гораздо в меньшей степени переобучена. Попробуем добавить деревьев для нее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат случайного леса с ohe, второй степенью умножения признаков и максимальной глубиной 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.006033</td>\n",
       "      <td>0.363033</td>\n",
       "      <td>0.475396</td>\n",
       "      <td>0.101362</td>\n",
       "      <td>500</td>\n",
       "      <td>{'rf__n_estimators': 500}</td>\n",
       "      <td>0.614876</td>\n",
       "      <td>0.601137</td>\n",
       "      <td>0.634841</td>\n",
       "      <td>0.616951</td>\n",
       "      <td>0.013837</td>\n",
       "      <td>1</td>\n",
       "      <td>0.658598</td>\n",
       "      <td>0.656364</td>\n",
       "      <td>0.649529</td>\n",
       "      <td>0.654830</td>\n",
       "      <td>0.003858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.907519</td>\n",
       "      <td>1.248304</td>\n",
       "      <td>0.872073</td>\n",
       "      <td>0.137757</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'rf__n_estimators': 1000}</td>\n",
       "      <td>0.610879</td>\n",
       "      <td>0.599671</td>\n",
       "      <td>0.631405</td>\n",
       "      <td>0.613985</td>\n",
       "      <td>0.013140</td>\n",
       "      <td>2</td>\n",
       "      <td>0.655696</td>\n",
       "      <td>0.656520</td>\n",
       "      <td>0.650441</td>\n",
       "      <td>0.654219</td>\n",
       "      <td>0.002693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.343335</td>\n",
       "      <td>0.042633</td>\n",
       "      <td>0.071172</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>60</td>\n",
       "      <td>{'rf__n_estimators': 60}</td>\n",
       "      <td>0.622517</td>\n",
       "      <td>0.589786</td>\n",
       "      <td>0.618930</td>\n",
       "      <td>0.610411</td>\n",
       "      <td>0.014657</td>\n",
       "      <td>3</td>\n",
       "      <td>0.655588</td>\n",
       "      <td>0.651259</td>\n",
       "      <td>0.644786</td>\n",
       "      <td>0.650544</td>\n",
       "      <td>0.004439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.873235</td>\n",
       "      <td>0.095360</td>\n",
       "      <td>0.102834</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>100</td>\n",
       "      <td>{'rf__n_estimators': 100}</td>\n",
       "      <td>0.607261</td>\n",
       "      <td>0.596201</td>\n",
       "      <td>0.625105</td>\n",
       "      <td>0.609523</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>4</td>\n",
       "      <td>0.647230</td>\n",
       "      <td>0.652640</td>\n",
       "      <td>0.649957</td>\n",
       "      <td>0.649942</td>\n",
       "      <td>0.002209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.511399</td>\n",
       "      <td>0.042128</td>\n",
       "      <td>0.034036</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>10</td>\n",
       "      <td>{'rf__n_estimators': 10}</td>\n",
       "      <td>0.605136</td>\n",
       "      <td>0.579984</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.600648</td>\n",
       "      <td>0.015370</td>\n",
       "      <td>5</td>\n",
       "      <td>0.649738</td>\n",
       "      <td>0.637161</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.637300</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3      20.006033      0.363033         0.475396        0.101362   \n",
       "4      39.907519      1.248304         0.872073        0.137757   \n",
       "1       2.343335      0.042633         0.071172        0.002542   \n",
       "2       3.873235      0.095360         0.102834        0.002660   \n",
       "0       0.511399      0.042128         0.034036        0.002741   \n",
       "\n",
       "  param_rf__n_estimators                      params  split0_test_score  \\\n",
       "3                    500   {'rf__n_estimators': 500}           0.614876   \n",
       "4                   1000  {'rf__n_estimators': 1000}           0.610879   \n",
       "1                     60    {'rf__n_estimators': 60}           0.622517   \n",
       "2                    100   {'rf__n_estimators': 100}           0.607261   \n",
       "0                     10    {'rf__n_estimators': 10}           0.605136   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "3           0.601137           0.634841         0.616951        0.013837   \n",
       "4           0.599671           0.631405         0.613985        0.013140   \n",
       "1           0.589786           0.618930         0.610411        0.014657   \n",
       "2           0.596201           0.625105         0.609523        0.011908   \n",
       "0           0.579984           0.616822         0.600648        0.015370   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "3                1            0.658598            0.656364   \n",
       "4                2            0.655696            0.656520   \n",
       "1                3            0.655588            0.651259   \n",
       "2                4            0.647230            0.652640   \n",
       "0                5            0.649738            0.637161   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \n",
       "3            0.649529          0.654830         0.003858  \n",
       "4            0.650441          0.654219         0.002693  \n",
       "1            0.644786          0.650544         0.004439  \n",
       "2            0.649957          0.649942         0.002209  \n",
       "0            0.625000          0.637300         0.010100  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('upsampler', SMOTE()),\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('rf', RandomForestClassifier(max_depth=6, random_state=1235))]\n",
    "    )\n",
    "param = {'rf__n_estimators': [10, 60, 100, 500, 1000]}\n",
    "\n",
    "grid_rf = GridSearchCV(model, param, scoring='f1', return_train_score=True, refit=False, cv=folders)\n",
    "grid_rf.fit(features_train, target_train)\n",
    "print('Результат случайного леса с ohe, второй степенью умножения признаков и максимальной глубиной 6')\n",
    "pd.DataFrame(grid_rf.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница между моделью с 60 оценщиками и 1000 оценщиками почти незаметна. Зато скорость обучения отличается во много раз. Глубина 6, число оценщиков 60 и умножение признаков – лучшие гиперпараметры случайного леса в нашем случае.\n",
    "Перейдем к случайным соседям.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат метода ближайших соседей с target_encoder\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_knn__n_neighbors</th>\n",
       "      <th>param_knn__weights</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.211464</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.575965</td>\n",
       "      <td>0.004740</td>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'knn__n_neighbors': 10, 'knn__weights': 'unif...</td>\n",
       "      <td>0.511965</td>\n",
       "      <td>0.521674</td>\n",
       "      <td>0.530882</td>\n",
       "      <td>0.521507</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>1</td>\n",
       "      <td>0.655926</td>\n",
       "      <td>0.664986</td>\n",
       "      <td>0.659869</td>\n",
       "      <td>0.660260</td>\n",
       "      <td>0.003709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.193512</td>\n",
       "      <td>0.015643</td>\n",
       "      <td>0.314951</td>\n",
       "      <td>0.022498</td>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'knn__n_neighbors': 10, 'knn__weights': 'dist...</td>\n",
       "      <td>0.504144</td>\n",
       "      <td>0.519051</td>\n",
       "      <td>0.519031</td>\n",
       "      <td>0.514075</td>\n",
       "      <td>0.007023</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.212508</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.259641</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'knn__n_neighbors': 5, 'knn__weights': 'dista...</td>\n",
       "      <td>0.508621</td>\n",
       "      <td>0.499644</td>\n",
       "      <td>0.501065</td>\n",
       "      <td>0.503110</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.166813</td>\n",
       "      <td>0.035064</td>\n",
       "      <td>0.427084</td>\n",
       "      <td>0.065005</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'knn__n_neighbors': 5, 'knn__weights': 'unifo...</td>\n",
       "      <td>0.501728</td>\n",
       "      <td>0.509368</td>\n",
       "      <td>0.496833</td>\n",
       "      <td>0.502643</td>\n",
       "      <td>0.005158</td>\n",
       "      <td>4</td>\n",
       "      <td>0.713938</td>\n",
       "      <td>0.711386</td>\n",
       "      <td>0.717931</td>\n",
       "      <td>0.714418</td>\n",
       "      <td>0.002693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.205953</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.473013</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'knn__n_neighbors': 3, 'knn__weights': 'unifo...</td>\n",
       "      <td>0.502610</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>0.504006</td>\n",
       "      <td>0.496060</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>5</td>\n",
       "      <td>0.782544</td>\n",
       "      <td>0.774549</td>\n",
       "      <td>0.784444</td>\n",
       "      <td>0.780513</td>\n",
       "      <td>0.004287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210001</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.222660</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'knn__n_neighbors': 3, 'knn__weights': 'dista...</td>\n",
       "      <td>0.496318</td>\n",
       "      <td>0.486567</td>\n",
       "      <td>0.501471</td>\n",
       "      <td>0.494785</td>\n",
       "      <td>0.006180</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.211334</td>\n",
       "      <td>0.008998</td>\n",
       "      <td>0.177745</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'knn__n_neighbors': 1, 'knn__weights': 'dista...</td>\n",
       "      <td>0.474832</td>\n",
       "      <td>0.470289</td>\n",
       "      <td>0.491143</td>\n",
       "      <td>0.478755</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.242931</td>\n",
       "      <td>0.047005</td>\n",
       "      <td>0.428270</td>\n",
       "      <td>0.010885</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'knn__n_neighbors': 1, 'knn__weights': 'unifo...</td>\n",
       "      <td>0.484493</td>\n",
       "      <td>0.451667</td>\n",
       "      <td>0.474110</td>\n",
       "      <td>0.470090</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "6       0.211464      0.006500         0.575965        0.004740   \n",
       "7       0.193512      0.015643         0.314951        0.022498   \n",
       "5       0.212508      0.006174         0.259641        0.002323   \n",
       "4       0.166813      0.035064         0.427084        0.065005   \n",
       "2       0.205953      0.002346         0.473013        0.006399   \n",
       "3       0.210001      0.002534         0.222660        0.004724   \n",
       "1       0.211334      0.008998         0.177745        0.004981   \n",
       "0       0.242931      0.047005         0.428270        0.010885   \n",
       "\n",
       "  param_knn__n_neighbors param_knn__weights  \\\n",
       "6                     10            uniform   \n",
       "7                     10           distance   \n",
       "5                      5           distance   \n",
       "4                      5            uniform   \n",
       "2                      3            uniform   \n",
       "3                      3           distance   \n",
       "1                      1           distance   \n",
       "0                      1            uniform   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "6  {'knn__n_neighbors': 10, 'knn__weights': 'unif...           0.511965   \n",
       "7  {'knn__n_neighbors': 10, 'knn__weights': 'dist...           0.504144   \n",
       "5  {'knn__n_neighbors': 5, 'knn__weights': 'dista...           0.508621   \n",
       "4  {'knn__n_neighbors': 5, 'knn__weights': 'unifo...           0.501728   \n",
       "2  {'knn__n_neighbors': 3, 'knn__weights': 'unifo...           0.502610   \n",
       "3  {'knn__n_neighbors': 3, 'knn__weights': 'dista...           0.496318   \n",
       "1  {'knn__n_neighbors': 1, 'knn__weights': 'dista...           0.474832   \n",
       "0  {'knn__n_neighbors': 1, 'knn__weights': 'unifo...           0.484493   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "6           0.521674           0.530882         0.521507        0.007724   \n",
       "7           0.519051           0.519031         0.514075        0.007023   \n",
       "5           0.499644           0.501065         0.503110        0.003940   \n",
       "4           0.509368           0.496833         0.502643        0.005158   \n",
       "2           0.481565           0.504006         0.496060        0.010265   \n",
       "3           0.486567           0.501471         0.494785        0.006180   \n",
       "1           0.470289           0.491143         0.478755        0.008954   \n",
       "0           0.451667           0.474110         0.470090        0.013699   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "6                1            0.655926            0.664986   \n",
       "7                2            1.000000            1.000000   \n",
       "5                3            1.000000            1.000000   \n",
       "4                4            0.713938            0.711386   \n",
       "2                5            0.782544            0.774549   \n",
       "3                6            1.000000            1.000000   \n",
       "1                7            1.000000            1.000000   \n",
       "0                8            1.000000            1.000000   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \n",
       "6            0.659869          0.660260         0.003709  \n",
       "7            1.000000          1.000000         0.000000  \n",
       "5            1.000000          1.000000         0.000000  \n",
       "4            0.717931          0.714418         0.002693  \n",
       "2            0.784444          0.780513         0.004287  \n",
       "3            1.000000          1.000000         0.000000  \n",
       "1            1.000000          1.000000         0.000000  \n",
       "0            1.000000          1.000000         0.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_target),\n",
    "     ('upsampler', SMOTE()),\n",
    "    ('knn', KNeighborsClassifier())]\n",
    "    )\n",
    "param = {\n",
    "    'knn__n_neighbors': [1, 3, 5, 10],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    }\n",
    "\n",
    "grid_knn = GridSearchCV(model, param, scoring='f1', return_train_score=True, refit=False, cv=folders)\n",
    "grid_knn.fit(features_train, target_train)\n",
    "print('Результат метода ближайших соседей с target_encoder')\n",
    "pd.DataFrame(grid_knn.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат метода ближайших соседей с OHE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_knn__n_neighbors</th>\n",
       "      <th>param_knn__weights</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.211997</td>\n",
       "      <td>0.072204</td>\n",
       "      <td>0.738882</td>\n",
       "      <td>0.046369</td>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'knn__n_neighbors': 10, 'knn__weights': 'unif...</td>\n",
       "      <td>0.539143</td>\n",
       "      <td>0.517922</td>\n",
       "      <td>0.544533</td>\n",
       "      <td>0.533866</td>\n",
       "      <td>0.011487</td>\n",
       "      <td>1</td>\n",
       "      <td>0.662148</td>\n",
       "      <td>0.675530</td>\n",
       "      <td>0.667391</td>\n",
       "      <td>0.668356</td>\n",
       "      <td>0.005506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.152035</td>\n",
       "      <td>0.014554</td>\n",
       "      <td>0.488528</td>\n",
       "      <td>0.056914</td>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'knn__n_neighbors': 10, 'knn__weights': 'dist...</td>\n",
       "      <td>0.513608</td>\n",
       "      <td>0.524177</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.529836</td>\n",
       "      <td>0.016067</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.139432</td>\n",
       "      <td>0.023955</td>\n",
       "      <td>0.550592</td>\n",
       "      <td>0.111490</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'knn__n_neighbors': 5, 'knn__weights': 'unifo...</td>\n",
       "      <td>0.513570</td>\n",
       "      <td>0.516364</td>\n",
       "      <td>0.535739</td>\n",
       "      <td>0.521891</td>\n",
       "      <td>0.009858</td>\n",
       "      <td>3</td>\n",
       "      <td>0.719178</td>\n",
       "      <td>0.729560</td>\n",
       "      <td>0.723152</td>\n",
       "      <td>0.723963</td>\n",
       "      <td>0.004277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.154964</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.379357</td>\n",
       "      <td>0.019168</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'knn__n_neighbors': 5, 'knn__weights': 'dista...</td>\n",
       "      <td>0.511989</td>\n",
       "      <td>0.508208</td>\n",
       "      <td>0.533904</td>\n",
       "      <td>0.518034</td>\n",
       "      <td>0.011328</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.145374</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>0.475768</td>\n",
       "      <td>0.050377</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'knn__n_neighbors': 3, 'knn__weights': 'unifo...</td>\n",
       "      <td>0.502618</td>\n",
       "      <td>0.516425</td>\n",
       "      <td>0.523529</td>\n",
       "      <td>0.514191</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>5</td>\n",
       "      <td>0.799550</td>\n",
       "      <td>0.793014</td>\n",
       "      <td>0.796572</td>\n",
       "      <td>0.796379</td>\n",
       "      <td>0.002672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.153467</td>\n",
       "      <td>0.014230</td>\n",
       "      <td>0.321036</td>\n",
       "      <td>0.029892</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'knn__n_neighbors': 3, 'knn__weights': 'dista...</td>\n",
       "      <td>0.507913</td>\n",
       "      <td>0.508527</td>\n",
       "      <td>0.510324</td>\n",
       "      <td>0.508921</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138781</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>0.199768</td>\n",
       "      <td>0.037928</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'knn__n_neighbors': 1, 'knn__weights': 'dista...</td>\n",
       "      <td>0.505226</td>\n",
       "      <td>0.485166</td>\n",
       "      <td>0.504595</td>\n",
       "      <td>0.498329</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.178012</td>\n",
       "      <td>0.023716</td>\n",
       "      <td>0.462671</td>\n",
       "      <td>0.027977</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'knn__n_neighbors': 1, 'knn__weights': 'unifo...</td>\n",
       "      <td>0.489726</td>\n",
       "      <td>0.491630</td>\n",
       "      <td>0.499579</td>\n",
       "      <td>0.493645</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "6       0.211997      0.072204         0.738882        0.046369   \n",
       "7       0.152035      0.014554         0.488528        0.056914   \n",
       "4       0.139432      0.023955         0.550592        0.111490   \n",
       "5       0.154964      0.010483         0.379357        0.019168   \n",
       "2       0.145374      0.012578         0.475768        0.050377   \n",
       "3       0.153467      0.014230         0.321036        0.029892   \n",
       "1       0.138781      0.010733         0.199768        0.037928   \n",
       "0       0.178012      0.023716         0.462671        0.027977   \n",
       "\n",
       "  param_knn__n_neighbors param_knn__weights  \\\n",
       "6                     10            uniform   \n",
       "7                     10           distance   \n",
       "4                      5            uniform   \n",
       "5                      5           distance   \n",
       "2                      3            uniform   \n",
       "3                      3           distance   \n",
       "1                      1           distance   \n",
       "0                      1            uniform   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "6  {'knn__n_neighbors': 10, 'knn__weights': 'unif...           0.539143   \n",
       "7  {'knn__n_neighbors': 10, 'knn__weights': 'dist...           0.513608   \n",
       "4  {'knn__n_neighbors': 5, 'knn__weights': 'unifo...           0.513570   \n",
       "5  {'knn__n_neighbors': 5, 'knn__weights': 'dista...           0.511989   \n",
       "2  {'knn__n_neighbors': 3, 'knn__weights': 'unifo...           0.502618   \n",
       "3  {'knn__n_neighbors': 3, 'knn__weights': 'dista...           0.507913   \n",
       "1  {'knn__n_neighbors': 1, 'knn__weights': 'dista...           0.505226   \n",
       "0  {'knn__n_neighbors': 1, 'knn__weights': 'unifo...           0.489726   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "6           0.517922           0.544533         0.533866        0.011487   \n",
       "7           0.524177           0.551724         0.529836        0.016067   \n",
       "4           0.516364           0.535739         0.521891        0.009858   \n",
       "5           0.508208           0.533904         0.518034        0.011328   \n",
       "2           0.516425           0.523529         0.514191        0.008682   \n",
       "3           0.508527           0.510324         0.508921        0.001023   \n",
       "1           0.485166           0.504595         0.498329        0.009311   \n",
       "0           0.491630           0.499579         0.493645        0.004268   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "6                1            0.662148            0.675530   \n",
       "7                2            1.000000            1.000000   \n",
       "4                3            0.719178            0.729560   \n",
       "5                4            1.000000            1.000000   \n",
       "2                5            0.799550            0.793014   \n",
       "3                6            1.000000            1.000000   \n",
       "1                7            1.000000            1.000000   \n",
       "0                8            1.000000            1.000000   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \n",
       "6            0.667391          0.668356         0.005506  \n",
       "7            1.000000          1.000000         0.000000  \n",
       "4            0.723152          0.723963         0.004277  \n",
       "5            1.000000          1.000000         0.000000  \n",
       "2            0.796572          0.796379         0.002672  \n",
       "3            1.000000          1.000000         0.000000  \n",
       "1            1.000000          1.000000         0.000000  \n",
       "0            1.000000          1.000000         0.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('upsampler', SMOTE()),\n",
    "    ('knn', KNeighborsClassifier())]\n",
    "    )\n",
    "param = {\n",
    "    'knn__n_neighbors': [1, 3, 5, 10],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    }\n",
    "\n",
    "grid_knn = GridSearchCV(model, param, scoring='f1', return_train_score=True, refit=False, cv=folders)\n",
    "grid_knn.fit(features_train, target_train)\n",
    "print('Результат метода ближайших соседей с OHE')\n",
    "pd.DataFrame(grid_knn.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что upsampling почти не повлиял на f1. Модель при этом показывает высокую переобученность. Не понятно, можно ли ее улучшить. Как самостоятельная модель она вряд ли полезна, но, возможно, в стэкинге пригодится. К нему и переходим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модели для первого уровня\n",
    "lr = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('upsampler', SMOTE()),\n",
    "    ('poly', PolynomialFeatures(degree=3)),\n",
    "    ('lr', LogisticRegression(penalty='l1', C=0.1, random_state=1235, solver='liblinear', max_iter=1000))]\n",
    "    )\n",
    "\n",
    "rf = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('upsampler', SMOTE()),\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('rf', RandomForestClassifier(60, max_depth=6, random_state=1235))])\n",
    "\n",
    "knn = Pipeline(\n",
    "    [('transformer', ct_ohe),\n",
    "    ('upsampler', SMOTE()),\n",
    "    ('knn', KNeighborsClassifier(10))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат стэкинга\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fit_time         91.265268\n",
       "score_time        0.769775\n",
       "test_f1           0.598512\n",
       "train_f1          0.640260\n",
       "test_roc_auc      0.861237\n",
       "train_roc_auc     0.893546\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stack_model = StackingClassifier(\n",
    "    [('lr', lr),\n",
    "    ('rf', rf),\n",
    "    ('knn', knn)]\n",
    ")\n",
    "print('Результат стэкинга')\n",
    "display(pd.DataFrame(cross_validate(stack_model, features_train, target_train, scoring=['f1', 'roc_auc'], return_train_score=True)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это тот случай, когда объединение моделей не дало положительного эффекта.\n",
    "Еще один вариант улучшения модели – подбор порогового значения принятия решения. Попробуем этот подход. Для начала создадим функцию, которая будет на основе кросс-валидации проверять разные пороговые значения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ThresholdCearchCV(model, features, target, threshold_levels, cv):\n",
    "    result = {}\n",
    "    for level in threshold_levels:\n",
    "        level_result = []\n",
    "        for train, test  in cv.split(features, target):\n",
    "            model.fit(features.iloc[train, :], target.iloc[train])\n",
    "            prob = model.predict_proba(features.iloc[test, :])[:, 1]\n",
    "            level_result .append(f1_score(target.iloc[test], (prob > level / 100)))\n",
    "        result[level] = level_result\n",
    "    return pd.DataFrame(result).agg(['mean', 'std']).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала попробуем изменить пороговое значение для выбранной нами модели случайного леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.500089</td>\n",
       "      <td>0.018025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.505901</td>\n",
       "      <td>0.011418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.524690</td>\n",
       "      <td>0.010358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.523970</td>\n",
       "      <td>0.012482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.531092</td>\n",
       "      <td>0.006460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.541083</td>\n",
       "      <td>0.009162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.547372</td>\n",
       "      <td>0.010645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.552854</td>\n",
       "      <td>0.006392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.557247</td>\n",
       "      <td>0.017012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.564521</td>\n",
       "      <td>0.015149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.573698</td>\n",
       "      <td>0.010908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.576949</td>\n",
       "      <td>0.010182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.579514</td>\n",
       "      <td>0.015571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.587963</td>\n",
       "      <td>0.012454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.591744</td>\n",
       "      <td>0.010341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.605417</td>\n",
       "      <td>0.014542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.602380</td>\n",
       "      <td>0.020015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.606611</td>\n",
       "      <td>0.018830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.607652</td>\n",
       "      <td>0.018170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.607709</td>\n",
       "      <td>0.023237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.611109</td>\n",
       "      <td>0.025162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.613159</td>\n",
       "      <td>0.024833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.610478</td>\n",
       "      <td>0.014306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.611694</td>\n",
       "      <td>0.021772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.611672</td>\n",
       "      <td>0.014604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.599565</td>\n",
       "      <td>0.016548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.603440</td>\n",
       "      <td>0.005963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.605965</td>\n",
       "      <td>0.009710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.598932</td>\n",
       "      <td>0.011023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.595687</td>\n",
       "      <td>0.009222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.588387</td>\n",
       "      <td>0.016192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.007069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.581823</td>\n",
       "      <td>0.009906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.580073</td>\n",
       "      <td>0.010422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.573152</td>\n",
       "      <td>0.003168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.569475</td>\n",
       "      <td>0.004510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.564678</td>\n",
       "      <td>0.010726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.557111</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.545922</td>\n",
       "      <td>0.006498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.542737</td>\n",
       "      <td>0.007925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.005309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean       std\n",
       "30  0.500089  0.018025\n",
       "31  0.505901  0.011418\n",
       "32  0.524690  0.010358\n",
       "33  0.523970  0.012482\n",
       "34  0.531092  0.006460\n",
       "35  0.541083  0.009162\n",
       "36  0.547372  0.010645\n",
       "37  0.552854  0.006392\n",
       "38  0.557247  0.017012\n",
       "39  0.564521  0.015149\n",
       "40  0.573698  0.010908\n",
       "41  0.576949  0.010182\n",
       "42  0.579514  0.015571\n",
       "43  0.587963  0.012454\n",
       "44  0.591744  0.010341\n",
       "45  0.605417  0.014542\n",
       "46  0.602380  0.020015\n",
       "47  0.606611  0.018830\n",
       "48  0.607652  0.018170\n",
       "49  0.607709  0.023237\n",
       "50  0.611109  0.025162\n",
       "51  0.613159  0.024833\n",
       "52  0.610478  0.014306\n",
       "53  0.611694  0.021772\n",
       "54  0.611672  0.014604\n",
       "55  0.599565  0.016548\n",
       "56  0.603440  0.005963\n",
       "57  0.605965  0.009710\n",
       "58  0.598932  0.011023\n",
       "59  0.595687  0.009222\n",
       "60  0.588387  0.016192\n",
       "61  0.591400  0.007069\n",
       "62  0.581823  0.009906\n",
       "63  0.580073  0.010422\n",
       "64  0.573152  0.003168\n",
       "65  0.569475  0.004510\n",
       "66  0.564678  0.010726\n",
       "67  0.557111  0.003882\n",
       "68  0.545922  0.006498\n",
       "69  0.542737  0.007925\n",
       "70  0.534884  0.005309"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThresholdCearchCV(rf, features_train, target_train, range(30, 71), folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь значимых улучшений добиться не удалось. Попробуем подвигать уровень принятия решения для логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.585793</td>\n",
       "      <td>0.012920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.591284</td>\n",
       "      <td>0.010538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.604440</td>\n",
       "      <td>0.015824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.607288</td>\n",
       "      <td>0.011627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.612486</td>\n",
       "      <td>0.009470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.610804</td>\n",
       "      <td>0.015712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.615436</td>\n",
       "      <td>0.010941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.607045</td>\n",
       "      <td>0.012349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.615012</td>\n",
       "      <td>0.011382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.604818</td>\n",
       "      <td>0.012424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.599288</td>\n",
       "      <td>0.011692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean       std\n",
       "40  0.585793  0.012920\n",
       "43  0.591284  0.010538\n",
       "46  0.604440  0.015824\n",
       "49  0.607288  0.011627\n",
       "52  0.612486  0.009470\n",
       "55  0.610804  0.015712\n",
       "58  0.615436  0.010941\n",
       "61  0.607045  0.012349\n",
       "64  0.615012  0.011382\n",
       "67  0.604818  0.012424\n",
       "70  0.599288  0.011692"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThresholdCearchCV(lr, features_train, target_train, range(40, 71, 3), folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно пороговое значение в районе 0,55-0,65 позволит повысить качество модели. Попробуем поискать с меньшим шагом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.616045</td>\n",
       "      <td>0.014583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.613703</td>\n",
       "      <td>0.010005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.608427</td>\n",
       "      <td>0.012354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.612066</td>\n",
       "      <td>0.014845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.608414</td>\n",
       "      <td>0.012431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.614890</td>\n",
       "      <td>0.012178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.605942</td>\n",
       "      <td>0.009786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.607229</td>\n",
       "      <td>0.016334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.605953</td>\n",
       "      <td>0.012813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.610184</td>\n",
       "      <td>0.010497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.608369</td>\n",
       "      <td>0.017054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean       std\n",
       "55  0.616045  0.014583\n",
       "56  0.613703  0.010005\n",
       "57  0.608427  0.012354\n",
       "58  0.612066  0.014845\n",
       "59  0.608414  0.012431\n",
       "60  0.614890  0.012178\n",
       "61  0.605942  0.009786\n",
       "62  0.607229  0.016334\n",
       "63  0.605953  0.012813\n",
       "64  0.610184  0.010497\n",
       "65  0.608369  0.017054"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThresholdCearchCV(lr, features_train, target_train, range(55, 66), folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стабильного улучшения качества модели по сравнению с логистической регрессией при обычном пороговом значении мы не наблюдаем. Значит оставим значение по умолчанию.\n",
    "*Выводы*\n",
    "В этой части проекта мы добавили к моделям расширение обучающей выборки. Это позволило немного улучшить результаты моделей, избавившись от дисбаланса. Модель KNN не позволила достичь хороших результатов. После подбора ряда гиперпараметров и перемножения признаков случайный лес и логистическая регрессия показали практически одинаковые результаты около 0,61. Можно было выбрать модель и с большей метрикой f1, но из-за явного переобучения есть сомнения в их обобщающей способности. Также мы попробовали повысить качество модели с помощью изменения порогового значения, но к изменению качества это не привело.\n",
    "На отложенной тестовой выборке мы будем проверять модель, базирующуюся на случайном лесе. Она показывает немного лучшую метрику качества и в несколько раз быстрее обучается.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что лучшей моделью стал случайный лес с максимальной глубиной 6 и 60 оценщиками. Также при подготовки данных эта модель расширяет обучающую выборку так, чтобы ликвидировать дисбаланс, а признаки перемножаются друг с другом, создавая дополнительные варианты разделения ветвей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика f1 на тестовой выборке равна 0.6227417640807651\n"
     ]
    }
   ],
   "source": [
    "rf.fit(features_train, target_train)\n",
    "print('Метрика f1 на тестовой выборке равна', f1_score(target_test, rf.predict(features_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "Мы получили данные о клиентах банка. Цель наших моделей – предсказание, кто из клиентов уйдет. \n",
    "На первом этапе мы рассмотрели данные. Три столбца явно не влияют на поведение клиентов и были удалены. Еще в одном столбце пришлось дополнять пропущенные данные. Главным установленным фактом стал дисбаланс в целевых показателях.\n",
    "На втором этапе мы проверили работу трех моделей, ничего не делая с дисбалансом. Применялся метод кросс-валидации, а также разные методы кодирования признаков. Достичь необходимого качества моделей не удалось.\n",
    "На третьем этапе мы расширили выборку путем увеличения объектов с положительным целевым показателем. Это позволило серьезно повысить f1для всех моделей. Определив две наиболее перспективные, мы подобрали для них гиперпараметры. Также попробовали воспользоваться стекингом. В результате было определено, что лучшей является модель случайного леса с максимальной глубиной 6, 60 оценщиками и перемножением признаков.\n",
    "На заключительном этапе лучшая из моделей была проверена на тестовой выборке. Она подтвердила полученный ранее показатель f1, превышающий 0,62. Таким образом поставленная заказчиком задача превзойти f1 равный 0,58 была решена.\n",
    "Отметим, что градиентный бустинг и нейронные сети позволяют значительно улучшить этот результат, однако по условию задания их использование было запрещено.\n"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1533,
    "start_time": "2022-10-26T09:59:06.881Z"
   },
   {
    "duration": 2,
    "start_time": "2022-10-26T09:59:08.416Z"
   },
   {
    "duration": 179,
    "start_time": "2022-10-26T09:59:08.419Z"
   },
   {
    "duration": 3,
    "start_time": "2022-10-26T09:59:08.599Z"
   },
   {
    "duration": 8,
    "start_time": "2022-10-26T09:59:08.603Z"
   },
   {
    "duration": 15,
    "start_time": "2022-10-26T09:59:08.612Z"
   },
   {
    "duration": 12,
    "start_time": "2022-10-26T09:59:08.628Z"
   },
   {
    "duration": 11,
    "start_time": "2022-10-26T09:59:08.642Z"
   },
   {
    "duration": 4,
    "start_time": "2022-10-26T09:59:08.655Z"
   },
   {
    "duration": 16,
    "start_time": "2022-10-26T09:59:08.661Z"
   },
   {
    "duration": 10665,
    "start_time": "2022-10-26T09:59:08.678Z"
   },
   {
    "duration": 8,
    "start_time": "2022-10-26T09:59:19.345Z"
   },
   {
    "duration": 37,
    "start_time": "2022-10-26T09:59:19.354Z"
   },
   {
    "duration": 13,
    "start_time": "2022-10-26T09:59:19.392Z"
   },
   {
    "duration": 7,
    "start_time": "2022-10-26T09:59:19.406Z"
   },
   {
    "duration": 10,
    "start_time": "2022-10-26T09:59:19.415Z"
   },
   {
    "duration": 16,
    "start_time": "2022-10-26T09:59:19.426Z"
   },
   {
    "duration": 20,
    "start_time": "2022-10-26T09:59:19.444Z"
   },
   {
    "duration": 2249,
    "start_time": "2022-10-26T09:59:19.465Z"
   },
   {
    "duration": 10144,
    "start_time": "2022-10-26T09:59:21.808Z"
   },
   {
    "duration": 152,
    "start_time": "2022-10-26T09:59:31.954Z"
   },
   {
    "duration": 1923,
    "start_time": "2022-10-26T09:59:32.107Z"
   },
   {
    "duration": 7,
    "start_time": "2022-10-26T09:59:34.031Z"
   },
   {
    "duration": 9,
    "start_time": "2022-10-26T09:59:34.039Z"
   },
   {
    "duration": 159,
    "start_time": "2022-10-26T09:59:34.049Z"
   },
   {
    "duration": 1065,
    "start_time": "2022-10-26T09:59:34.210Z"
   },
   {
    "duration": 31,
    "start_time": "2022-10-26T09:59:35.277Z"
   },
   {
    "duration": 765,
    "start_time": "2022-10-26T09:59:35.311Z"
   },
   {
    "duration": 227438,
    "start_time": "2022-10-26T09:59:36.078Z"
   },
   {
    "duration": 33791,
    "start_time": "2022-10-26T10:03:23.517Z"
   },
   {
    "duration": 16597,
    "start_time": "2022-10-26T10:03:57.310Z"
   },
   {
    "duration": 167,
    "start_time": "2022-10-26T10:04:13.908Z"
   },
   {
    "duration": 5039,
    "start_time": "2022-10-26T10:04:14.077Z"
   },
   {
    "duration": 280,
    "start_time": "2022-10-26T10:04:19.118Z"
   },
   {
    "duration": 215,
    "start_time": "2022-10-26T10:04:19.400Z"
   },
   {
    "duration": 6260,
    "start_time": "2022-10-26T10:04:19.617Z"
   },
   {
    "duration": 35,
    "start_time": "2022-10-26T10:04:25.879Z"
   },
   {
    "duration": 105,
    "start_time": "2022-10-26T10:04:25.915Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
